[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis and Visualization in Python",
    "section": "",
    "text": "Data Analysis and Visualization in Python (IN2339)\nThis is the lecture script of the module Data Analysis and Visualization in Python (IN2339).\n\nNote — work in progress\nThis book is an adaptation to the programming language Python of the original script Data Analysis and Visualization in R and is still under development. Chapters will be added in order. For any content not yet implemented, please refer to the original.\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)\n\n\nAcknowledgments\nThis script has been first put together in the winter semester 2020/2021 by Felix Brechtmann, Alexander Karollus, Daniela Klaproth-Andrade, Pedro Silva, and Julien Gagneur with help from Xueqi Cao, Laura Martens, Ines Scheller, Vangelis Theodorakis, and Vicente Yépez.\nWe leveraged work from colleagues who helped creating lecture slides since 2017: Žiga Avsec, Ines Assum, Daniel Bader, Jun Cheng, Bašak Eraslan, Mathias Heinig, Jan Krumsieck, Christian Mertes, and Georg Stricker.\nThe script has been adapted from R to Python in the winter semester 2025/2026 by XXX.\n\n\nPrerequisites\nBasics in probabilities are required. Chapters 13-15 (“Introduction to Statistics with R”, “Probability” and “Random variables”) of the Book “Introduction to Data Science” https://rafalab.github.io/dsbook/ make a good refresher. Make sure all concepts are familiar to you. Check your knowledge by trying the exercises.\n\n\nDatasets\nDatasets used in this script are available to download as a compressed file here.\n\n\nFeedback\nFor improvement suggestions, reporting errors and typos, please use the online document here.",
    "crumbs": [
      "Data Analysis and Visualization in Python (IN2339)"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html",
    "href": "topic00_Introduction/script/script_00.html",
    "title": "Introduction",
    "section": "",
    "text": "Data Science: What and why?\nData science is an interdisciplinary field about processes and systems to extract knowledge or insights from data. The goals of Data Science include discovering new phenomena or trends from data, enabling decisions based on facts derived from data, and communicating findings from data. It is a continuation of some of the data analysis fields such as statistics, data mining, and predictive analytics.\nData Science is at the heart of the scientific method, which starts with making data-driven observations to formulate testable hypotheses. It furthermore comes into play to visualize and assess experimental results. Data science skills are therefore necessary to any field of scientific research. Data science is the main tools of epidemiology, the study of health and disease in populations, which largely relies on observational data. Moreover data science is important in the industry, to understand operational process, and in business analytics, to understand a particular market. Hence, with the rise of big data in all areas of society, data science skills are some of the most demanded skills on the job market. Last, but not least, in an era of fake news, data science skills are important for citizens of modern societies.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#what-you-will-learn-and-not-learn",
    "href": "topic00_Introduction/script/script_00.html#what-you-will-learn-and-not-learn",
    "title": "Introduction",
    "section": "What you will learn and not learn",
    "text": "What you will learn and not learn\nThe goal of this course is to provide you with general analytic techniques to extract knowledge, patterns, and connections between samples and variables, and how to communicate these in an intuitive and clear way.\nThis course focuses on front-end data science. This means, it teaches practical skills to analyse data. We will focus on tidy data, visualizations, and data manipulation in Python. To only then dive into the math required to understand and interpret analysis results.\nThis course does not teach back-end data science, i.e. it does not teach how to develop your own statistical or machine learning models, nor how to develop scalable data processing software.\nOther courses offered by the faculty of Informatics cover data science back-end skills.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#the-python-language",
    "href": "topic00_Introduction/script/script_00.html#the-python-language",
    "title": "Introduction",
    "section": "The Python language",
    "text": "The Python language\nPython is a versatile high-level programming language used intensively for data analytics and machine learning. It is a great language for front-end data science, i.e. to rapidly manipulate, visualize and come to raising interesting hypotheses.\nBut thanks to its versatility, even complicated back-end software, such as deep-learning models, can be designed and maintained with Python. There is always a trade-off between abstraction from the computer language — i.e., legibility for humans — and the efficiency of programming languages. As Python is a high-level language, its purpose is to reduce time spent coding and to maximize users’ time looking at and thinking about the data, rather than minimizing the computer’s running time. Because researchers and data analysts often need to make quick, effective analyses and usually don’t have time to optimize low-level code, Python offers an ideal compromise between these two worlds.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#course-overview",
    "href": "topic00_Introduction/script/script_00.html#course-overview",
    "title": "Introduction",
    "section": "Course overview",
    "text": "Course overview\nThe lecture is structured into three main parts covering the major steps of data analysis:\n\nGet the data: After basic introduction to Python, learn how to fetch and manipulate real-world datasets. How to structure them to most conveniently work with them (tidy data).\nLook at the data: Basic and advanced visualization techniques allows navigating large and complex datasets, identifying interesting signal, and formulating hypotheses. Typical sources of confounding are discussed. Recommendation to present an analysis in compelling fashion are also given.\nConclude: Concepts of hypothesis testing will allow concluding about the statistical robustness of discovered associations. Also, methods from supervised learning will allow to model data and build accurate predictors.\n\nThe chapters of this script corresponds to individual lectures. Appendices provide further technical details as well as Python tricks and tips.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#complementary-reading",
    "href": "topic00_Introduction/script/script_00.html#complementary-reading",
    "title": "Introduction",
    "section": "Complementary reading",
    "text": "Complementary reading\n— TODO —\nThese books offer complementary information to this script:\n\nIntroduction to Data Science, Rafael A. Irizarry [https://rafalab.github.io/dsbook/]\nR for Data Science, Garrett Grolemund and Hadley Wickham [https://r4ds.had.co.nz/]\nStatistical Inference via Data Science, Chester Ismay and Albert Y. Kim [https://moderndive.com/]\nFundamentals of Data Visualization, Claus O. Wilke [https://clauswilke.com/dataviz/]\nAdvanced R, Hadley Wickham [https://adv-r.hadley.nz/]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html",
    "href": "topic01_Python_Basics/script/script_01.html",
    "title": "1  Python Basics for Data Science",
    "section": "",
    "text": "1.1 Python Environments\nThis chapter provides a quick introduction to the programming language Python and common tools used for data analysis.\nWhile you can run Python from a system terminal, for data analysis, it’s highly recommended to use an Integrated Development Environment (IDE) or a notebook environment. These tools provide features like code completion, debugging, and interactive data exploration.\nPopular choices include:\nThese environments help organize your work by providing panes for writing scripts, an interactive console to execute code, a variable explorer to inspect objects, and areas to view plots and documentation.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#python-environments",
    "href": "topic01_Python_Basics/script/script_01.html#python-environments",
    "title": "1  Python Basics for Data Science",
    "section": "",
    "text": "Google Colaboratory (Colab): A free, cloud-based Jupyter Notebook environment that requires no setup and runs entirely in the browser. It’s an excellent choice for beginners and for collaborative projects, as notebooks can be easily shared like Google Docs. It also provides free access to powerful hardware like GPUs, making it ideal for machine learning and data science tasks.\nJupyter Notebook/JupyterLab: A web-based interactive environment that allows you to create documents containing live code, equations, visualizations, and narrative text. This format is excellent for exploratory data analysis and sharing results. It can be run on your local machine.\nVisual Studio Code (VS Code): A powerful, free code editor with excellent Python and Jupyter Notebook support through its extensions. It combines the features of a traditional IDE with the interactivity of notebooks, making it a great choice for projects that involve both scripting and data exploration.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#first-steps-with-python",
    "href": "topic01_Python_Basics/script/script_01.html#first-steps-with-python",
    "title": "1  Python Basics for Data Science",
    "section": "1.2 First Steps With Python",
    "text": "1.2 First Steps With Python\nThis section is inspired by the book Introduction to Data Science by Rafael Irizarry, adapted for Python.\n\n1.2.1 Objects\nSuppose a high school student asks for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). The quadratic formula gives us the solutions:\n\\[\n\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\]\nwhich change depending on the values of \\(a\\), \\(b\\), and \\(c\\). A key advantage of programming is that we can define variables and write expressions that solve these equations numerically. If we are asked to solve \\(x^2 + x -1 = 0\\), we first define our variables:\n\nimport numpy as np\n\na = 1\nb = 1\nc = -1\n\nHere, we use = to assign values to variables. This stores the values for later use. Note that Python doesn’t print anything when we make this assignment, which means the objects were defined successfully.\nTo see the value stored in a variable, you can simply type its name in an interactive console or notebook cell, or use the print function:\n\na\n\n1\n\n\nA more explicit way is to use print():\n\nprint(a)\n\n1\n\n\nWe use the term object to describe things stored in Python, such as variables, functions, and more complex data structures.\n\n\n1.2.2 The Namespace\nAs we define objects, we are changing the current namespace. In an interactive environment like Jupyter or IPython, you can see the user-defined variables using the magic command %whos.\n\n%whos\n\nVariable     Type        Data/Info\n----------------------------------\na            int         1\nb            int         1\nc            int         -1\nnp           module      &lt;module 'numpy' from '/Us&lt;...&gt;kages/numpy/__init__.py'&gt;\nojs_define   function    &lt;function ojs_define at 0x1126671a0&gt;\n\n\nYou should see a, b, and c. If you try to access a variable that hasn’t been defined, you’ll get an error. For example, typing x will result in a NameError.\nNow that our variables are defined, we can use the quadratic formula to find the solutions: We have to import numpy module to use the sqrt function from it.\n\nimport numpy as np\n\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nprint(solution_1)\nprint(solution_2)\n\n0.6180339887498949\n-1.618033988749895\n\n\n\n\n1.2.3 Functions\nOnce you define variables, the data analysis process can usually be described as a series of functions applied to data. Python has many built-in functions, and countless more are available through external libraries like NumPy, Pandas, and Scikit-learn.\nWe already used print() and np.sqrt(). In general, we use parentheses to call a function. If you type a function’s name without parentheses, you’ll see a reference to the function object itself, not its result. There are many more prebuilt functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nMost functions take one or more arguments. For example, the np.log() function calculates the natural logarithm of a number:\n\nnp.log(8)\n\nnp.float64(2.0794415416798357)\n\n\nYou can find out what a function does and what arguments it expects by using the built-in help() function or, in an interactive environment, by typing a question mark ? after the function name. The function round() returns the closest integer to a given number.\n\nhelp(round)\n\nThe help page shows that round() can take a second argument ndigits for the precision. Arguments can be passed by position or by name (keyword arguments).\n\nround(3.1415, 2)\n\n3.14\n\n\n\nround(3.1415, ndigits=2)\n\n3.14\n\n\nWhen using keyword arguments, the order doesn’t matter:\n\nround(ndigits=2, number=3.1415)\n\n3.14\n\n\nArithmetic and relational operators (+, -, *, /, ^, ==, &gt;) are also fundamental. Note that in Python, the power operator is **, not ^.\n\n2**3\n\n8\n\n\n\n\n1.2.4 Variable Names\nWe’ve used a, b, and c, but variable names can be more descriptive. Python variable names must start with a letter or underscore, can contain numbers, but cannot contain spaces, and should not be variables that are predefined in Python. For example, don’t name one of your variables list by typing something like list = 2.\nA common convention in Python is to use snake_case for variable names: use meaningful words, all lowercase, separated by underscores.\n\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nFor more advice, we highly recommend studying PEP8 style guide1.\n\n\n1.2.5 Reproducible Analysis\nOne of the most powerful aspects of programming for data science is the ability to create reproducible analyses. This means that your work can be easily repeated, verified, and modified by yourself or others at any point in the future.\n\nWriting Code That Runs Reliably\nWhen working with notebooks or interactive environments, it’s crucial to write code that produces the same results every time it’s executed. Here are some key principles:\nExecute cells in order: Always run your notebook cells from top to bottom in sequential order. Variables and functions defined in earlier cells are needed by later cells. If you jump around or skip cells, you might get unexpected errors or incorrect results.\nAvoid state-dependent operations: Write code so that running a cell multiple times doesn’t break your analysis. For example:\n# BAD: This will give different results each time you run it\nx = 10\nx = x + 1  # If you run this cell twice, x becomes 12, then 13, etc.\nprint(x)\n# GOOD: This always gives the same result\nx = 10\ny = x + 1  # No matter how many times you run this, y is always 11\nprint(y)\n\n\nThe Power of Reusable Code\nTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\n\na = 3\nb = 2\nc = -1\n\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nprint(solution_1, solution_2)\n\n0.3333333333333333 -1.0\n\n\nBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\n\n\n\n1.2.6 Commenting Your Code\nAny line in Python that starts with a hash symbol # is a comment and is not executed. Use comments to explain what your code is doing.\n\n# Code to compute solution to a quadratic equation of the form ax^2 + bx + c\n\n# Define the variables\na = 3 \nb = 2\nc = -1\n\n# Now compute the solution\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nLonger comments, e.g., documentation, can be wrapped with triple quotes (““” or ’’’).",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#data-types",
    "href": "topic01_Python_Basics/script/script_01.html#data-types",
    "title": "1  Python Basics for Data Science",
    "section": "1.3 Data Types",
    "text": "1.3 Data Types\nVariables in Python can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The type() function helps determine an object’s type.\n\na = 2\ntype(a)\n\nint\n\n\nTo work efficiently, it’s important to understand Python’s basic data types.\n\n1.3.1 Numbers\nPython can handle integers (int) and floating-point numbers (float):\n\nx = 7\ny = 3.14\nprint(type(x))\nprint(type(y))\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n\n\n\n\n1.3.2 Strings\nText is represented as strings (str), written in quotes:\n\nname = \"Python\"\nprint(type(name))\n\n&lt;class 'str'&gt;\n\n\nYou can combine strings with the + operator:\n\ngreeting = \"Hello \" + name\nprint(greeting)\n\nHello Python\n\n\n\n\n1.3.3 Lists\nA list is an ordered, mutable collection of items, which can be of different types. You access elements using their zero-based index.\nrecord2 = [\"John Doe\", 1234]\nprint(record2[0]) # Access the first element\n\n\n1.3.4 Dictionaries\nA dictionary (dict) stores key-value pairs and is similar to a named list in R.\nrecord = {\n\"name\": \"John Doe\",\n\"student_id\": 1234,\n\"grades\": [95, 82, 91, 97, 93],\n\"final_grade\": \"A\"\n}\nThis dictionary contains a string, an integer, a list of numbers, and another string.\nprint(record)\nprint(type(record))\nYou access elements in a dictionary using their keys in square brackets:\nrecord['student_id']\n\n\n1.3.5 Boolean\nAnother important data type is a boolean (bool), which can only be True or False. Relational operators like &lt;, &gt;, ==, &lt;=, &gt;= produce booleans.\n\nz = 3 == 2\nprint(z)\nprint(type(z))\n\nFalse\n&lt;class 'bool'&gt;\n\n\nHere the == is a relational operator asking if 3 is equal to 2. In Python, if you just use one =, you actually assign a variable, but if you use two == you test for equality.\n\n\n1.3.6 DataFrames\nThe most common way to store a tabular dataset in Python is in a DataFrame, which is the primary data structure in the pandas library. A DataFrame is a two-dimensional table where rows represent observations and columns represent variables.\nAs an example, let’s use the tips dataset, which records restaurant bills and tips, along with information about the server, day, and time. The dataset is available directly through the Seaborn library.\n\nimport pandas as pd\nimport seaborn as sns\n\n# Load the dataset\ntips = sns.load_dataset(\"tips\")\n\nWe can check the object’s type to confirm that it’s a DataFrame:\n\ntype(tips)\n\npandas.core.frame.DataFrame\n\n\nYou can quickly preview the first few rows of the data using the head() method:\n\ntips.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\nTo access one of the columns, use its name in the access operator []\n\ntips['time']\n\n0      Dinner\n1      Dinner\n2      Dinner\n3      Dinner\n4      Dinner\n        ...  \n239    Dinner\n240    Dinner\n241    Dinner\n242    Dinner\n243    Dinner\nName: time, Length: 244, dtype: category\nCategories (2, object): ['Lunch', 'Dinner']\n\n\n\n\n1.3.7 Categorical Data\nIn the tips dataset, several columns (like sex, smoker, day, and time) contain categorical data — values that fall into a small set of distinct groups rather than continuous numbers.\nPandas provides a special category data type that is more memory-efficient and allows you to store an explicit order for these categories.\n\nimport seaborn as sns\nimport pandas as pd\n\n# Load example dataset\ntips = sns.load_dataset('tips')\n\n# Check which columns are categorical\ntips.dtypes\n\ntotal_bill     float64\ntip            float64\nsex           category\nsmoker        category\nday           category\ntime          category\nsize             int64\ndtype: object\n\n\nLet’s look at one categorical column — day (the day of the week the tip was recorded).\n\n# Convert 'day' to category type\ntips['day'] = tips['day'].astype('category')\ntips['day'].dtype\n\nCategoricalDtype(categories=['Thur', 'Fri', 'Sat', 'Sun'], ordered=False, categories_dtype=object)\n\n\nWe can inspect the unique categories using the .cat.categories accessor:\n\ntips['day'].cat.categories\n\nIndex(['Thur', 'Fri', 'Sat', 'Sun'], dtype='object')\n\n\nBy default, categories have no particular order. But we can reorder them to reflect the natural order of the week.\n\n# Define a meaningful order for the days\nordered_days = ['Thur', 'Fri', 'Sat', 'Sun']\n\n# Re-categorize 'day' with an explicit order\ntips['day'] = pd.Categorical(tips['day'],\n                             categories=ordered_days,\n                             ordered=True)\n\n# Check the new order\ntips['day'].cat.categories\n\nIndex(['Thur', 'Fri', 'Sat', 'Sun'], dtype='object')\n\n\n\n\n1.3.8 NumPy Arrays\nNumPy arrays are the fundamental object for numerical computing in Python. They are list-like or grid-like structures where all elements must be of the same type. While pandas DataFrames are great for heterogeneous, labeled data, NumPy arrays are optimized for homogeneous, numerical array operations. Many pandas operations use NumPy arrays under the hood. Vectorized operations on NumPy arrays are significantly faster than native Python loops due to highly optimized implementation.\nWe can create a NumPy array (often called a matrix when it’s 2D) as follows. We’ll need to import the numpy library, conventionally as np. Use the arange function to create an array with sequential numbers; reshape allows you to rearrange multidimensional arrays. For example, here the initially “flat” array will be shaped into a matrix by filling rows from left to right.\n\nimport numpy as np\n\nmat = np.arange(1, 13).reshape(4, 3)\nprint(mat)\n\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n\n\nYou can check the type of the element stored in the array with dtype.\n\nmat.dtype\n\ndtype('int64')\n\n\nTo convert it to another type, use astype.\n\nmat_float = mat.astype(np.float64)\nmat_float.dtype\n\ndtype('float64')\n\n\nYou can access specific elements using [row, column] indexing, remembering that Python uses 0-based indexing.\n\n# Access element in the second row, third column\nmat[1, 2]\n\nnp.int64(6)\n\n\nTo get the entire second row, use a colon : for the column index:\n\n# Get the second row (index 1)\nmat[1, :]\n\narray([4, 5, 6])\n\n\nSimilarly, to get the entire third column:\n\n# Get the third column (index 2)\nmat[:, 2]\n\narray([ 3,  6,  9, 12])\n\n\nYou can access more than one column or more than one row if you like. This will give you a new matrix. Note, that the limits are specified as interval and the third colun won’t be included.\n\nmat[:, 1:3]\n\narray([[ 2,  3],\n       [ 5,  6],\n       [ 8,  9],\n       [11, 12]])\n\n\nYou can subset both rows and columns:\n\nmat[1:3, 2:4]\n\narray([[6],\n       [9]])\n\n\nIndices in python are “wrapped” around the end of the array, so you can use negative indices. For example, a handy way to get the last row would be: You can subset both rows and columns:\n\nmat[-1]\n\narray([10, 11, 12])\n\n\nIn general, the access operator arr[i:j:k] accepts 3 arguments. This means take every kth element from ith to jth (excluding the end). Omitting i or j means that you want to take the corresponding end of the array.\n\narr = np.arange(10)\narr[1:-1:2]\n\narray([1, 3, 5, 7])\n\n\nWe can convert a NumPy array into a pandas DataFrame:\n\npd.DataFrame(mat, columns=['A', 'B', 'C'])\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n2\n7\n8\n9\n\n\n3\n10\n11\n12\n\n\n\n\n\n\n\n\n\n1.3.9 List And Arrays: References And Copies\nList and arrays variables are just references. Always copy lists and arrays when reassigning! Compare:\n\na = [1, 2, 3]\nb = a\nb[1] = 5\nprint(a)\n\n[1, 5, 3]\n\n\n\na = [1, 2, 3]\nb = a[:]\nb[1] = 5\nprint(a)\n\n[1, 2, 3]\n\n\n\n\n1.3.10 Ranges\nYou can create sequences of numbers using Python’s built-in range() function or NumPy’s np.arange() function, which is more flexible as it allows for non-integer steps.\n\n# Integers from 1 up to (but not including) 11\nlist(range(1, 11))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n# A sequence from 1 to 10 with a step of 2\nlist(range(1, 11, 2))\n\n[1, 3, 5, 7, 9]\n\n\n\n# A sequence with floating point steps using numpy\nnp.arange(1, 10, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. ,\n       7.5, 8. , 8.5, 9. , 9.5])\n\n\n\n\n1.3.11 Non numerical and infinity (nan, inf)\nWhen working with numerical data, certain operations can render some of the cells invalid. Numpy will store np.nan, or np.inf in that case. One of the common examples is division by 0:\n\nnp.int64(1) / np.int64(0)\n\nnp.float64(inf)\n\n\nNumPy uses a highly optimized underlying implementation, so the types there look more complicated, for example int64 instead of just int in native Python. This just means that this type uses 64 bits, whereas in native Python the int type is not restricted.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#numpy-operation",
    "href": "topic01_Python_Basics/script/script_01.html#numpy-operation",
    "title": "1  Python Basics for Data Science",
    "section": "1.4 Numpy operation",
    "text": "1.4 Numpy operation\nNumpy library comes with a handful set of operation that help working with data. Let’s load a murders dataset. This dataset includes gun murder data for US states in 2010.\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv'\nmurders = pd.read_csv(url)\nmurders.head()\n\n\n\n\n\n\n\n\nstate\nabb\nregion\npopulation\ntotal\n\n\n\n\n0\nAlabama\nAL\nSouth\n4779736\n135\n\n\n1\nAlaska\nAK\nWest\n710231\n19\n\n\n2\nArizona\nAZ\nWest\n6392017\n232\n\n\n3\nArkansas\nAR\nSouth\n2915918\n93\n\n\n4\nCalifornia\nCA\nWest\n37253956\n1257\n\n\n\n\n\n\n\nWe will be working with individual columns, converted to vectors. To convert a column to a NumPy array, use the .values attribute after accessing a column. Note: it’s not a function call, so parentheses are not needed.\n\nmurders['state'].values\n\narray(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)\n\n\n\n1.4.1 np.sort()\nSay we want to sort the vector, use .sort() operation.\n\ntotal = murders['total'].values\ntotal = np.sort(total)\ntotal[:10]\n\narray([ 2,  4,  5,  5,  7,  8, 11, 12, 12, 16])\n\n\n\n\n1.4.2 .max() and .argmax()\nIf we only want the largest value, we can use .max(). To get the index of the largest value, we use .argmax().\n\n# Get the maximum value in the 'total' column\nmax_total = murders['total'].values.max()\nprint(max_total)\n\n# Get the index of the state with the maximum total\ni_max = murders['total'].values.argmax()\n\n# Use that index to verify\nmurders['total'].values[i_max]\n\n# Find out which state has the most murders.\n# Note: the order of entries when getting .values is the same\n\nmurders['state'].values[i_max]\n\n1257\n\n\n'California'\n\n\n\n\n1.4.3 `.sum()``\nCompute a sum of the array by using the sum function.\n\nmurders['total'].values.sum()\n\nnp.int64(9403)\n\n\n\n\n1.4.4 Vectorized Arithmetics\nCalifornia has the most murders, but it also has the largest population. To compare safety, we should look at the murder rate per capita. The powerful vectorized arithmetic capabilities of NumPy make this easy.\n\n\n1.4.5 Operations on a vectors\nArithmetic operations on NumPy arrays are applied element-wise.\n\ninches = np.array([69, 62, 66, 70, 70, 73, 67, 73, 67, 70])\ncentimeters = inches * 2.54\nprint(centimeters)\n\n[175.26 157.48 167.64 177.8  177.8  185.42 170.18 185.42 170.18 177.8 ]\n\n\nSimilarly, we can subtract a single number from every element:\n\ninches - 69\n\narray([ 0, -7, -3,  1,  1,  4, -2,  4, -2,  1])\n\n\n\n\n1.4.6 Operations between two arrays\nIf we perform an operation between two arrays of the same length, the operation is performed element-wise, matching elements by their index.\nThis means that to compute the murder rate per 100,000 people, we can simply divide the total array by the population array and multiply by 100,000.\n\nmurder_rate = murders['total'].values / murders['population'].values * 100_000\n\nNote that the number 100000 in Python is written with an underscore to improve readability; Python supports both options.\nNow we can add this rate as a new column to our DataFrame and sort by it to get a more meaningful ranking of state safety.\n\nmurders['rate'] = murder_rate\nidx_max_rate = murder_rate.argmax()\nmurders['state'].values[idx_max_rate]\n\n'District of Columbia'\n\n\n\n\n1.4.7 Boolean Indexing\nWe can use a logical condition to create a boolean arrays (True/False values). For example, let’s find states with a murder rate less than 0.71.\n\nind = murders['rate'].values &lt; 0.71\nind.dtype\n\ndtype('bool')\n\n\nWe can then use this boolean array inside square brackets [] to filter any other array of the same size, or even the whole dataframem keeping only the rows where the condition is True.\n\nmurders['state'][ind]\n\n11           Hawaii\n15             Iowa\n29    New Hampshire\n34     North Dakota\n45          Vermont\nName: state, dtype: object\n\n\nTo count how many states meet this condition, we can take the sum() of the boolean array, since True is treated as 1 and False as 0.\n\nind.sum()\n\nnp.int64(5)\n\n\n\n\n1.4.8 Logical Operators\nWe can combine multiple logical conditions using the operators & (and), | (or), and ~ (not). For example, to find safe states in the West region (murder rate &lt;= 1):\n# Note the parentheses around each condition, which are required\nis_west = murders['region'].values == 'West'\nis_safe = murders['rate'].values &lt;= 1\n\nind = is_west & is_safe\nmurders['state'][ind]",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#python-programming-basics",
    "href": "topic01_Python_Basics/script/script_01.html#python-programming-basics",
    "title": "1  Python Basics for Data Science",
    "section": "1.5 Python Programming Basics",
    "text": "1.5 Python Programming Basics\nSo far we’ve focused on using Python interactively for data analysis. But Python is also a powerful general-purpose programming language. The key building blocks for writing more complex programs and automating tasks are:\n\nConditionals (if, elif, else): To execute code only if a certain condition is met.\nLoops (for, while): To repeat a block of code multiple times.\nFunctions (def): To bundle code into reusable, named blocks.\n\nThese concepts allow you to build complex data analysis pipelines, create custom tools, and move from simple scripts to sophisticated programs.\n\n1.5.1 A Simple Loop Example\nLet’s see a basic example of a for loop that solves multiple quadratic equations:\n\n# Solve multiple quadratic equations using a loop\nequations = [\n    {\"a\": 1, \"b\": 1, \"c\": -1},    # x^2 + x - 1 = 0\n    {\"a\": 2, \"b\": -3, \"c\": 1},   # 2x^2 - 3x + 1 = 0\n    {\"a\": 1, \"b\": 0, \"c\": -4}    # x^2 - 4 = 0\n]\n\nfor i, eq in enumerate(equations):\n    a, b, c = eq[\"a\"], eq[\"b\"], eq[\"c\"]\n    discriminant = b**2 - 4*a*c\n    \n    if discriminant &gt;= 0:\n        solution_1 = (-b + np.sqrt(discriminant)) / (2*a)\n        solution_2 = (-b - np.sqrt(discriminant)) / (2*a)\n        print(f\"Equation {i+1}: {a}x² + {b}x + {c} = 0\")\n        print(f\"Solutions: x₁ = {solution_1:.3f}, x₂ = {solution_2:.3f}\")\n    else:\n        print(f\"Equation {i+1}: {a}x² + {b}x + {c} = 0\")\n        print(\"No real solutions (complex roots)\")\n    print(\"-\" * 40)\n\nEquation 1: 1x² + 1x + -1 = 0\nSolutions: x₁ = 0.618, x₂ = -1.618\n----------------------------------------\nEquation 2: 2x² + -3x + 1 = 0\nSolutions: x₁ = 1.000, x₂ = 0.500\n----------------------------------------\nEquation 3: 1x² + 0x + -4 = 0\nSolutions: x₁ = 2.000, x₂ = -2.000\n----------------------------------------\n\n\nThis loop demonstrates how we can process multiple datasets or perform repetitive calculations efficiently, rather than copying and pasting the same code multiple times.\nCreate your own functions using def. You can plot simple graphs using plotnine package.\n\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, theme_minimal, theme\n\ndef parabola(n):\n    if n &gt; 0 and type(n) == int:\n        # Create data for the parabola\n        x = np.array(range(-n, n+1))\n        y = x**2\n        \n        # Create a DataFrame for plotnine\n        df = pd.DataFrame({'x': x, 'y': y})\n        \n        # Create the plot using plotnine\n        plot = (ggplot(df, aes(x='x', y='y')) +\n                geom_line() +\n                theme_minimal() +\n                theme(figure_size=(4,3)))\n        \n        return plot\n\nparabola(10)",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#python-is-indentation-sensitive",
    "href": "topic01_Python_Basics/script/script_01.html#python-is-indentation-sensitive",
    "title": "1  Python Basics for Data Science",
    "section": "1.6 Python Is Indentation Sensitive",
    "text": "1.6 Python Is Indentation Sensitive\nIndentation defines block structure (e.g., inside functions, loops, conditionals). Mixing tabs and spaces is discouraged.\n\n# Correct indentation\nif True:\n    print(\"This is indented\")\n    if 1 &lt; 2:\n        print(\"Nested block\")\n\n# Incorrect indentation would raise an IndentationError\n\nThis is indented\nNested block",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#random-generation-examples-from-distributions",
    "href": "topic01_Python_Basics/script/script_01.html#random-generation-examples-from-distributions",
    "title": "1  Python Basics for Data Science",
    "section": "1.7 Random Generation Examples From Distributions",
    "text": "1.7 Random Generation Examples From Distributions\nIn this course, we often use simulated data to explore data visualization and modeling concepts. Generating our own data helps us focus on understanding patterns, relationships, and visualization techniques without worrying about the complexity or limitations of real datasets.\n\n# random integers\nnp.random.seed(0)\nprint('randints:', np.random.randint(low=0, high=10, size=5))\n\n# normal distribution (mean, std)\nprint('normal sample:', np.random.normal(loc=0.0, scale=1.0, size=5))\n\n# uniform\nprint('uniform:', np.random.rand(5))\n\n# choice with probabilities\nprint('choice:', np.random.choice(['red', 'green', 'blue'], size=5,\\\n    p=[0.1, 0.7, 0.2]))\n\nrandints: [5 0 3 3 7]\nnormal sample: [ 1.86755799 -0.97727788  0.95008842 -0.15135721 -0.10321885]\nuniform: [0.79172504 0.52889492 0.56804456 0.92559664 0.07103606]\nchoice: ['red' 'red' 'blue' 'green' 'blue']\n\n\n\nScripts vs. Jupyter Notebooks: Choosing the Right Tool\nWhen working with Python for data analysis, you have two main approaches: Python scripts (.py files) and Jupyter notebooks (.ipynb files). Each has its strengths and is suited for different types of work.\nPython Scripts (.py files): - Best for: Production code, automated pipelines, functions and modules, version control - Advantages: - Clean, linear execution from top to bottom - Easy to version control with Git (plain text format) - Excellent for creating reusable functions and modules - Faster execution and debugging - Better for automated workflows and deployment - Use cases: Data cleaning pipelines, analysis functions, automated reports, web applications\nJupyter Notebooks (.ipynb files): - Best for: Exploratory analysis, prototyping, teaching, presenting results - Advantages: - Interactive development with immediate feedback - Mix code, visualizations, and narrative text in one document - Cell-by-cell execution allows for iterative exploration - Excellent for data visualization and sharing insights - Built-in support for rich media (plots, tables, LaTeX) - Use cases: Data exploration, research documentation, tutorials, presentations",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#best-practices-for-reproducible-analysis",
    "href": "topic01_Python_Basics/script/script_01.html#best-practices-for-reproducible-analysis",
    "title": "1  Python Basics for Data Science",
    "section": "1.8 Best Practices for Reproducible Analysis",
    "text": "1.8 Best Practices for Reproducible Analysis\n\nStart with notebooks for exploration: Use Jupyter notebooks to explore your data, test hypotheses, and prototype your analysis.\nRefactor to scripts for production: Once your analysis is solid, extract reusable functions into Python scripts (.py files) that can be imported and used across projects.\nDocument your workflow: Whether using scripts or notebooks, include clear comments and markdown explanations of your methodology.\nUse version control: Track changes to your analysis over time using Git, especially for script files.\nMake your environment reproducible: Use tools like requirements.txt or conda environment files to specify exact package versions.\n\nThe goal is to create analyses that can be easily understood, modified, and reproduced by anyone (including your future self) who needs to work with your code. These concepts allow you to build complex data analysis pipelines, create custom tools, and move from simple scripts to sophisticated programs.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#footnotes",
    "href": "topic01_Python_Basics/script/script_01.html#footnotes",
    "title": "1  Python Basics for Data Science",
    "section": "",
    "text": "https://peps.python.org/pep-0008/↩︎",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html",
    "href": "topic02_Data_Wrangling/script/script_02.html",
    "title": "2  Data Wrangling",
    "section": "",
    "text": "2.1 DataFrames\nData wrangling refers to the task of processing raw data into useful formats. This chapter introduces basic data wrangling operations in Python using pandas.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#dataframes",
    "href": "topic02_Data_Wrangling/script/script_02.html#dataframes",
    "title": "2  Data Wrangling",
    "section": "",
    "text": "2.1.1 Overview\nIn Python, tabular data lives in a pandas DataFrame — a 2-D, labeled table with columns (variables) and rows (observations). Each column can have its own dtype (numeric, string, categorical, datetime, etc.). Rows are labeled by an index — an ordered set of labels that can be integers, strings, datetimes, or other hashable types\nA DataFrame is built on top of NumPy, so column-wise operations are vectorized and usually fast; many methods return a new DataFrame rather than modifying the original, while explicit assignment via .loc / .iloc updates selected values. You can always call .copy() when you want an independent object.\nIn this course we’ll use pandas DataFrame as our main tool for data manipulation. It offers a concise, readable API for filtering, grouping, joining, reshaping (wide/long), handling missing values, and working with time series. The payoff comes on two fronts:\n\nProgramming (expressive, chainable syntax; easy to read, debug and maintain)\nComputing (efficient, vectorized operations and memory efficiency)\n\nWe will now start with some basic operations and examples of using pandas. First of all, let us create and inspect some DataFrames to get a first impression.\n\n\n2.1.2 Creating and loading DataFrames\nTo create a DataFrame, we can use a dictionary, where each key corresponds to a column name. All the columns have to have the same length. If vectors of different lengths are provided upon creation of a DataFrame, you’ll get an error. Here is an example:\n\n# pip install pandas\ndf = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n    \"age\": [25, 30, 35],\n    \"city\": [\"Berlin\", \"Paris\", \"London\"]\n})\ndf\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nBerlin\n\n\n1\nBob\n30\nParis\n\n\n2\nCharlie\n35\nLondon\n\n\n\n\n3 rows × 3 columns\n\n\n\nIf we want to convert a numpy array or a list of lists or some other python objects to a DataFrame, all we have to do is to call the pd.DataFrame() constructor. You can also provide the names of the columns:\n\ndata = np.array([[100, 5], [80, 7]])\ndf = pd.DataFrame(data, columns = ['Speed', 'Time'])\ntype(df)\n\npandas.core.frame.DataFrame\n\n\nHere you can see that the function type() informs us that df is a pandas DataFrame.\nAlternatively, we can read files from disk and process them using DataFrame. The easiest way to do so is to use the functions pd.read_csv() or pd.read_excel(). Here is an example using a subset of the Kaggle flight and airports dataset that is limited to flights going in or to the Los Angeles airport. We refer to the description of the Kaggle flights and airports challenge for more details [https://www.kaggle.com/tylerx/flights-and-airports-data].\nWe provide the file flightsLAX.csv as part of our datasets (See Datasets). To run the following code, save the comma-separated value file flightsLAX.csv into a local folder of your choice and replace the string \"path_to_file\" with the actual path to your flightsLAX.csv file. For example \"path_to_file\" could be substituted with \"/Users/samantha/mydataviz_folder/extdata\".\n\nflights = pd.read_csv('path_to_file/flightsLAX.csv')\n\nTyping the name of the newly created DataFrame (flights) in a notebook cell displays its first and last rows. We observe that reading the file was successful.\n\nflights\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n0\n2015\n1\n1\n...\n263.0\n2330\n741.0\n\n\n1\n2015\n1\n1\n...\n258.0\n2342\n756.0\n\n\n2\n2015\n1\n1\n...\n228.0\n2125\n753.0\n\n\n3\n2015\n1\n1\n...\n188.0\n1535\n605.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n389365\n2015\n12\n31\n...\n291.0\n2345\n400.0\n\n\n389366\n2015\n12\n31\n...\n132.0\n954\n225.0\n\n\n389367\n2015\n12\n31\n...\n198.0\n1744\n544.0\n\n\n389368\n2015\n12\n31\n...\n272.0\n2611\n753.0\n\n\n\n\n389369 rows × 12 columns\n\n\n\nA first step in any analysis should involve inspecting the data we just read in. This often starts by looking at the first and last rows of the table as we did above. You can also use functions .head() and .tail() to return 5 first or 5 last rows of a DataFrame:\n\nflights.head() # flights.tail()\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n0\n2015\n1\n1\n...\n263.0\n2330\n741.0\n\n\n1\n2015\n1\n1\n...\n258.0\n2342\n756.0\n\n\n2\n2015\n1\n1\n...\n228.0\n2125\n753.0\n\n\n3\n2015\n1\n1\n...\n188.0\n1535\n605.0\n\n\n4\n2015\n1\n1\n...\n255.0\n2342\n839.0\n\n\n\n\n5 rows × 12 columns\n\n\n\n\n\n2.1.3 Inspecting tables\nAfter looking at the first and last rows of the table (using df.head() and df.tail()), we are often interested in the size of our data set, which we can extract as dimensions of the underlying array:\n\nflights.shape\n\n(389369, 12)\n\n\nAlternatively you can use len(flights) for num rows and len(flights.columns) for columns. Note, that index is not counted as a column.\nWe are also interested in columns of this DataFrame:\n\nflights.columns\n\nIndex(['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER',\n       'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DEPARTURE_TIME', 'AIR_TIME',\n       'DISTANCE', 'ARRIVAL_TIME'],\n      dtype='object')\n\n\nNext, we are often interested in basic statistics on the columns.\nTo obtain this information we can call the .describe() function on the table:\n\nflights.iloc[:, :6].describe()\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\nDAY_OF_WEEK\nFLIGHT_NUMBER\n\n\n\n\ncount\n389369.0\n389369.000000\n389369.000000\n389369.000000\n389369.000000\n\n\nmean\n2015.0\n6.197502\n15.703389\n3.934160\n1905.390165\n\n\nstd\n0.0\n3.366953\n8.779643\n1.996267\n1766.645447\n\n\nmin\n2015.0\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n25%\n2015.0\n3.000000\n8.000000\n2.000000\n501.000000\n\n\n50%\n2015.0\n6.000000\n16.000000\n4.000000\n1296.000000\n\n\n75%\n2015.0\n9.000000\n23.000000\n6.000000\n2617.000000\n\n\nmax\n2015.0\n12.000000\n31.000000\n7.000000\n6896.000000\n\n\n\n\n8 rows × 5 columns\n\n\n\nThis provides us already a lot of information about our data. We can for example see that all data is from 2015 as all values in the YEAR column are 2015.\nHowever, for categorical column basic statistics cannot be computed, so column AIRLINE is not in the output.To investigate categorical columns we can have a look at their unique elements using:\n\nflights['AIRLINE'].unique()\n\narray(['AA', 'US', 'DL', 'UA', 'OO', 'AS', 'B6', 'NK', 'VX', 'WN', 'HA',\n       'F9', 'MQ'], dtype=object)\n\n\nThis command provided us the airline identifiers present in the dataset. Another valuable information for categorical variables is how often each category occurs. This can be obtained using the following commands:\n\nflights['AIRLINE'].value_counts()\n\nAIRLINE\nWN    75022\nOO    73389\nAA    65483\nUA    54862\n      ...  \nUS     7374\nHA     3112\nF9     2770\nMQ      368\nName: count, Length: 13, dtype: int64",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#indices-and-row-subsetting",
    "href": "topic02_Data_Wrangling/script/script_02.html#indices-and-row-subsetting",
    "title": "2  Data Wrangling",
    "section": "2.2 Indices and row subsetting",
    "text": "2.2 Indices and row subsetting\n\n2.2.1 Indices in pandas\nEvery DataFrame has a row index and a column index.\n\nRow index: labels each row (default = 0, 1, 2, …)\n\n\nflights.index\n\nRangeIndex(start=0, stop=389369, step=1)\n\n\n\nColumn index: labels each column (the column names)\n\n\nflights.columns # note, that this function returns \"Index\" object\n\nIndex(['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE', 'FLIGHT_NUMBER',\n       'ORIGIN_AIRPORT', 'DESTINATION_AIRPORT', 'DEPARTURE_TIME', 'AIR_TIME',\n       'DISTANCE', 'ARRIVAL_TIME'],\n      dtype='object')\n\n\nRow index can be customized and any type: integers, strings, dates, tuples etc. Moreover, pandas will not control whether your index column contains only unique or ordered values, but unique, meaningful labels make data easier to work with. Index column can be used for row selection, row alignment when joining/combining tables, sorting and grouping.\nThe function set_index() lets us turn a column into the row index:\n\ndf = pd.DataFrame({\n    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"Peter\"],\n    \"age\": [25, 30, 35, 36],\n    \"city\": [\"Berlin\", \"Paris\", \"London\", \"Munich\"]\n})\n\ndf = df.set_index('name')\ndf\n\n\n\n\n\n\n\n\nage\ncity\n\n\nname\n\n\n\n\n\n\nAlice\n25\nBerlin\n\n\nBob\n30\nParis\n\n\nCharlie\n35\nLondon\n\n\nPeter\n36\nMunich\n\n\n\n\n4 rows × 2 columns\n\n\n\nThe function reset_index() moves the index back into a regular column and parameter names allows giving a custom name to the newly created column:\n\ndf.reset_index(names='name')\n\n\n\n\n\n\n\n\nname\nage\ncity\n\n\n\n\n0\nAlice\n25\nBerlin\n\n\n1\nBob\n30\nParis\n\n\n2\nCharlie\n35\nLondon\n\n\n3\nPeter\n36\nMunich\n\n\n\n\n4 rows × 3 columns\n\n\n\npandas allows flexibility in handling index column, which in practice can easily lead to errors. Therefore, in order to avoid mistakes in your code, we recommend keeping index default.\n\n\n2.2.2 Subsetting a single row\nIf we want to see the second row of the table, we can us .loc[] (by label) or .iloc[] (by position)\n\ndf.loc['Bob']   # Access the row with index value 'Bob'\n\nage        30\ncity    Paris\nName: Bob, Length: 2, dtype: object\n\n\n\ndf.iloc[1]   # Access the 2nd row \n\nage        30\ncity    Paris\nName: Bob, Length: 2, dtype: object\n\n\n\n\n2.2.3 Subsetting multiple rows\nFor accessing multiple consecutive rows we can use the start:stop syntax with iloc:\n\ndf.iloc[0:2]\n\n\n\n\n\n\n\n\nage\ncity\n\n\nname\n\n\n\n\n\n\nAlice\n25\nBerlin\n\n\nBob\n30\nParis\n\n\n\n\n2 rows × 2 columns\n\n\n\nAccessing multiple rows that are not necessarily consecutive can be done by creating a vector of int values:\n\ndf.iloc[[0, 2]]\n\n\n\n\n\n\n\n\nage\ncity\n\n\nname\n\n\n\n\n\n\nAlice\n25\nBerlin\n\n\nCharlie\n35\nLondon\n\n\n\n\n2 rows × 2 columns\n\n\n\nAccessing multiple rows by index can be done by creating a vector:\n\ndf.loc[[\"Alice\", \"Charlie\"]]\n\n\n\n\n\n\n\n\nage\ncity\n\n\nname\n\n\n\n\n\n\nAlice\n25\nBerlin\n\n\nCharlie\n35\nLondon\n\n\n\n\n2 rows × 2 columns\n\n\n\n\n\n2.2.4 Subsetting rows by logical conditions\nOften, a more useful way to subset rows is using logical conditions, using a logical vector instead of a vector of indices\n\nWe can create such logical vectors using the following binary operators:\n\n==\n&lt;\n&gt;\n!=\n%in%\n\n\nThe syntax is following table[condition].\nFor example, entries of flights arriving to Miami International Airport can be extracted using ==:\n\nflights[flights['DESTINATION_AIRPORT'] == 'MIA'] \n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n1\n2015\n1\n1\n...\n258.0\n2342\n756.0\n\n\n4\n2015\n1\n1\n...\n255.0\n2342\n839.0\n\n\n111\n2015\n1\n1\n...\n257.0\n2342\n1539.0\n\n\n164\n2015\n1\n1\n...\n264.0\n2342\n1627.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n388747\n2015\n12\n31\n...\n251.0\n2342\n1844.0\n\n\n388871\n2015\n12\n31\n...\n245.0\n2342\n2018.0\n\n\n389313\n2015\n12\n31\n...\n256.0\n2342\n528.0\n\n\n389364\n2015\n12\n31\n...\n250.0\n2342\n731.0\n\n\n\n\n3023 rows × 12 columns\n\n\n\nNote, that flights['DESTINATION_AIRPORT'] == 'MIA' is a so-called mask, that has the same length as the data frame and consists of True and False:\n\nflights['DESTINATION_AIRPORT']== 'MIA'\n\n0         False\n1          True\n2         False\n3         False\n          ...  \n389365    False\n389366    False\n389367    False\n389368    False\nName: DESTINATION_AIRPORT, Length: 389369, dtype: bool\n\n\nIf we are now interested in to get all flights that depart from New York area, we can use isin([list]):\n\nflights[flights['ORIGIN_AIRPORT'].isin(['JFK', 'EWR'])]\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n40\n2015\n1\n1\n...\n358.0\n2475\n951.0\n\n\n56\n2015\n1\n1\n...\n340.0\n2454\n1020.0\n\n\n57\n2015\n1\n1\n...\n360.0\n2475\n1027.0\n\n\n58\n2015\n1\n1\n...\n368.0\n2475\n1026.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n389246\n2015\n12\n31\n...\n336.0\n2454\n2233.0\n\n\n389251\n2015\n12\n31\n...\n349.0\n2475\n2305.0\n\n\n389299\n2015\n12\n31\n...\n352.0\n2475\n53.0\n\n\n389336\n2015\n12\n31\n...\n349.0\n2475\n143.0\n\n\n\n\n16619 rows × 12 columns\n\n\n\nWe can also concatenate multiple conditions using the logical OR | or the logical AND & operator. Always wrap conditions in parentheses!\n\nflights[(flights['ORIGIN_AIRPORT'].isin(['JFK', 'EWR'])) | (flights['DESTINATION_AIRPORT'].isin(['JFK', 'EWR']))  ]\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n25\n2015\n1\n1\n...\n263.0\n2454\n1350.0\n\n\n30\n2015\n1\n1\n...\n279.0\n2475\n1413.0\n\n\n40\n2015\n1\n1\n...\n358.0\n2475\n951.0\n\n\n53\n2015\n1\n1\n...\n274.0\n2475\n1458.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n389341\n2015\n12\n31\n...\n256.0\n2475\n625.0\n\n\n389345\n2015\n12\n31\n...\n255.0\n2454\n639.0\n\n\n389359\n2015\n12\n31\n...\n274.0\n2475\n748.0\n\n\n389361\n2015\n12\n31\n...\n251.0\n2454\n719.0\n\n\n\n\n33252 rows × 12 columns\n\n\n\nUsing and or or operators instead of will lead to error in this case.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#handling-missing-values",
    "href": "topic02_Data_Wrangling/script/script_02.html#handling-missing-values",
    "title": "2  Data Wrangling",
    "section": "2.3 Handling missing values",
    "text": "2.3 Handling missing values\nMissing values in pandas are encoded using np.nan for floats, NaT for datetime columns and pd.NA for all other data types. To check whether a value is missing, we cannot use equals-to (==) operator - instead one has to use .isna() or .notna(). In the following example we firstly set a value to np.nan:\n\nflights.loc[6, 'DISTANCE'] = np.nan\n\n\nflights[flights['DISTANCE'].isna()]\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n6\n2015\n1\n1\n...\n42.0\nNaN\n650.0\n\n\n\n\n1 rows × 12 columns",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#column-operations",
    "href": "topic02_Data_Wrangling/script/script_02.html#column-operations",
    "title": "2  Data Wrangling",
    "section": "2.4 Column operations",
    "text": "2.4 Column operations\n\n2.4.1 Introducing Series\nEach column of a DataFrame is a Series:\n\ndestination = flights['DESTINATION_AIRPORT']\ntype(destination)\n\npandas.core.series.Series\n\n\n\ndestination\n\n0         PBI\n1         MIA\n2         CLT\n3         MSP\n         ... \n389365    ANC\n389366    SEA\n389367    ORD\n389368    BOS\nName: DESTINATION_AIRPORT, Length: 389369, dtype: object\n\n\nA Series is a one-dimensional labeled array. It has two components:\n\nValues (['PBI', 'MIA', 'CLT', ..., 'BOS'])\nIndex ([0, 1, 2, ..., 389368])\n\nSeries supports operations like .mean(), .sum(), etc, which are applied to values of the object.\n\n\n2.4.2 Accessing one or multiple columns\nSince a DataFrame is just a collection of Series side by side, selecting a single column returns the corresponding Series. That’s how you can do it:\n\nflights['DESTINATION_AIRPORT'] \n\n0         PBI\n1         MIA\n2         CLT\n3         MSP\n         ... \n389365    ANC\n389366    SEA\n389367    ORD\n389368    BOS\nName: DESTINATION_AIRPORT, Length: 389369, dtype: object\n\n\nAttribute access (df.col) is convenient, but does not work if column names have spaces or conflict with methods (e.g. flights.DESTINATION_AIRPORT).\nAlthough feasible (using .iloc[]), it is not advisable to access a column by its number since the ordering or number of columns can easily change. Also, if you have a data set with a large number of columns (e.g. 50), how do you know which one is column 18? Therefore, we recommend to use the column names to access columns for preventing bugs and better readibility: flights['DESTINATION_AIRPORT'] instead of flights.iloc[, 8].\nTo access multiple columns, you provide their names as a list:\n\nairport_columns = ['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']\nflights[airport_columns]\n# or directly flights[['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']]\n\n\n\n\n\n\n\n\nORIGIN_AIRPORT\nDESTINATION_AIRPORT\n\n\n\n\n0\nLAX\nPBI\n\n\n1\nLAX\nMIA\n\n\n2\nLAX\nCLT\n\n\n3\nLAX\nMSP\n\n\n...\n...\n...\n\n\n389365\nLAX\nANC\n\n\n389366\nLAX\nSEA\n\n\n389367\nLAX\nORD\n\n\n389368\nLAX\nBOS\n\n\n\n\n389369 rows × 2 columns\n\n\n\nNote, that the returned object is a DataFrame.\nFor accessing a specific entry (i.e. specific column and specific row), we can use the following syntax:\n\nflights.loc[5, 'DESTINATION_AIRPORT']  # Access a specific cell.\n\n'IAH'",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#groupby-operation",
    "href": "topic02_Data_Wrangling/script/script_02.html#groupby-operation",
    "title": "2  Data Wrangling",
    "section": "2.5 Groupby operation",
    "text": "2.5 Groupby operation\nOften we want to compute summaries per category, for example: average flight time per airline or number of flights per day. For this we can use groupby(), that splits the data into groups. Note, that groupby() only defines the groups – you must follow it with an aggregation such as .mean(), .sum(), .count(), or .agg(…) to compute results.\n\nflights.groupby(\"AIRLINE\")[\"AIR_TIME\"].mean().head(5)\n\nAIRLINE\nAA    219.481334\nAS    141.018698\nB6    309.795684\nDL    207.072013\nF9    159.940407\nName: AIR_TIME, Length: 5, dtype: float64\n\n\nThe returned object is Series with unique \"AIRLINE\" values being in the index column.\nWe can also compute the number of rows in each group using .size() directly after groupby() (or .count() on any column):\n\nflights.groupby('AIRLINE').size()\n# flights.groupby('AIRLINE')[\"AIR_TIME\"].count() will give the same result\n\nAIRLINE\nAA    65483\nAS    16144\nB6     8216\nDL    50343\n      ...  \nUA    54862\nUS     7374\nVX    23598\nWN    75022\nLength: 13, dtype: int64\n\n\nInstead of selecting one column first, we can compute the mean of several numeric columns per group and the result will be a DataFrame:\n\nflights.groupby('AIRLINE')[['AIR_TIME', 'DISTANCE']].mean()\n\n\n\n\n\n\n\n\nAIR_TIME\nDISTANCE\n\n\nAIRLINE\n\n\n\n\n\n\nAA\n219.481334\n1739.233068\n\n\nAS\n141.018698\n1040.034006\n\n\nB6\n309.795684\n2486.148856\n\n\nDL\n207.072013\n1656.216475\n\n\n...\n...\n...\n\n\nUA\n211.620082\n1693.550381\n\n\nUS\n210.394884\n1658.258069\n\n\nVX\n185.363741\n1432.538393\n\n\nWN\n105.199757\n760.259284\n\n\n\n\n13 rows × 2 columns\n\n\n\nNote, that applying .mean() to non-numeric columns will result in an error.\nSometimes we want to apply several operation to the same column simultaneiusly. Instead of one summary per column, .agg() lets us compute several stats at once—per group.\n\n(\n    flights.groupby('AIRLINE').agg(\n      {\n          'AIR_TIME':  ['mean', 'std'],\n          'DISTANCE':  ['min'],\n          'DESTINATION_AIRPORT': ['size', 'first', 'nunique']\n      })\n)\n\n\n\n\n\n\n\n\nAIR_TIME\nDISTANCE\nDESTINATION_AIRPORT\n\n\n\nmean\nstd\nmin\nsize\nfirst\nnunique\n\n\nAIRLINE\n\n\n\n\n\n\n\n\n\n\nAA\n219.481334\n92.889719\n236.0\n65483\nPBI\n31\n\n\nAS\n141.018698\n51.806424\n590.0\n16144\nSEA\n7\n\n\nB6\n309.795684\n28.457740\n954.0\n8216\nJFK\n5\n\n\nDL\n207.072013\n88.908566\n109.0\n50343\nMSP\n31\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\nUA\n211.620082\n94.832456\n110.0\n54862\nIAH\n25\n\n\nUS\n210.394884\n105.224833\n370.0\n7374\nCLT\n9\n\n\nVX\n185.363741\n113.504572\n236.0\n23598\nLAX\n12\n\n\nWN\n105.199757\n69.257334\n236.0\n75022\nBNA\n28\n\n\n\n\n13 rows × 6 columns",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#sorting-operation",
    "href": "topic02_Data_Wrangling/script/script_02.html#sorting-operation",
    "title": "2  Data Wrangling",
    "section": "2.6 Sorting operation",
    "text": "2.6 Sorting operation",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#sorting-by-column",
    "href": "topic02_Data_Wrangling/script/script_02.html#sorting-by-column",
    "title": "2  Data Wrangling",
    "section": "2.7 Sorting by column",
    "text": "2.7 Sorting by column\nAnother common operation is sorting, that can be done using .sort_values(col_name) function:\n\nflights.sort_values('DISTANCE')\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n14288\n2015\n1\n13\n...\n24.0\n86.0\n1341.0\n\n\n91561\n2015\n3\n22\n...\n29.0\n86.0\n2146.0\n\n\n15571\n2015\n1\n14\n...\n23.0\n86.0\n1624.0\n\n\n12635\n2015\n1\n12\n...\n27.0\n86.0\n705.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10247\n2015\n1\n9\n...\n349.0\n2615.0\n2153.0\n\n\n324280\n2015\n11\n2\n...\n328.0\n2615.0\n2054.0\n\n\n50168\n2015\n2\n14\n...\n310.0\n2615.0\n625.0\n\n\n6\n2015\n1\n1\n...\n42.0\nNaN\n650.0\n\n\n\n\n389369 rows × 12 columns\n\n\n\nBy default sort_values() sorts the column in ascending order (from the smallest to the largest value), but we can specify order using ascending=...:\n\nflights.sort_values('DISTANCE', ascending = False)\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n169907\n2015\n5\n29\n...\n337.0\n2615.0\n1212.0\n\n\n138515\n2015\n5\n2\n...\n315.0\n2615.0\n1139.0\n\n\n378210\n2015\n12\n21\n...\n315.0\n2615.0\n646.0\n\n\n189410\n2015\n6\n14\n...\n301.0\n2615.0\n2029.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n60949\n2015\n2\n24\n...\n25.0\n86.0\n1230.0\n\n\n97950\n2015\n3\n28\n...\n24.0\n86.0\n1229.0\n\n\n92771\n2015\n3\n23\n...\n24.0\n86.0\n2141.0\n\n\n6\n2015\n1\n1\n...\n42.0\nNaN\n650.0\n\n\n\n\n389369 rows × 12 columns\n\n\n\nsort_values() function cannot be applied to index column, tehre we have to use sort_index():\n\nflights.set_index(\"FLIGHT_NUMBER\").sort_index()\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\nFLIGHT_NUMBER\n\n\n\n\n\n\n\n\n\n\n\n1\n2015\n1\n21\n...\n313.0\n2556.0\n1212.0\n\n\n1\n2015\n12\n7\n...\n322.0\n2475.0\n1145.0\n\n\n1\n2015\n6\n1\n...\n315.0\n2475.0\n1230.0\n\n\n1\n2015\n3\n13\n...\n367.0\n2556.0\n1153.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n6862\n2015\n12\n26\n...\nNaN\n715.0\nNaN\n\n\n6874\n2015\n12\n26\n...\n40.0\n236.0\n2308.0\n\n\n6876\n2015\n11\n26\n...\n108.0\n834.0\n734.0\n\n\n6896\n2015\n11\n26\n...\n129.0\n862.0\n1006.0\n\n\n\n\n389369 rows × 11 columns",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#extending-tables",
    "href": "topic02_Data_Wrangling/script/script_02.html#extending-tables",
    "title": "2  Data Wrangling",
    "section": "2.8 Extending tables",
    "text": "2.8 Extending tables\nWe can create new columns by performing operations on existing ones:\n\nflights[\"DIST_MILES\"] = flights[\"DISTANCE\"] * 0.621371\nflights.head()\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nDISTANCE\nARRIVAL_TIME\nDIST_MILES\n\n\n\n\n0\n2015\n1\n1\n...\n2330.0\n741.0\n1447.794430\n\n\n1\n2015\n1\n1\n...\n2342.0\n756.0\n1455.250882\n\n\n2\n2015\n1\n1\n...\n2125.0\n753.0\n1320.413375\n\n\n3\n2015\n1\n1\n...\n1535.0\n605.0\n953.804485\n\n\n4\n2015\n1\n1\n...\n2342.0\n839.0\n1455.250882\n\n\n\n\n5 rows × 13 columns\n\n\n\nWe can also combine columns to compute new values. Here we compute speed by dividing distance by air time:\n\nflights[\"SPEED_MPH\"] = (\n    flights[\"DIST_MILES\"] / flights[\"AIR_TIME\"] * 60\n)\nflights.head()\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nARRIVAL_TIME\nDIST_MILES\nSPEED_MPH\n\n\n\n\n0\n2015\n1\n1\n...\n741.0\n1447.794430\n330.295307\n\n\n1\n2015\n1\n1\n...\n756.0\n1455.250882\n338.430438\n\n\n2\n2015\n1\n1\n...\n753.0\n1320.413375\n347.477204\n\n\n3\n2015\n1\n1\n...\n605.0\n953.804485\n304.405687\n\n\n4\n2015\n1\n1\n...\n839.0\n1455.250882\n342.411972\n\n\n\n\n5 rows × 14 columns\n\n\n\nTo delete column, you can use .drop() function and provide column names as a list:\n\nflights.drop(columns = ['DIST_MILES', 'SPEED_MPH'])\n\n\n\n\n\n\n\n\nYEAR\nMONTH\nDAY\n...\nAIR_TIME\nDISTANCE\nARRIVAL_TIME\n\n\n\n\n0\n2015\n1\n1\n...\n263.0\n2330.0\n741.0\n\n\n1\n2015\n1\n1\n...\n258.0\n2342.0\n756.0\n\n\n2\n2015\n1\n1\n...\n228.0\n2125.0\n753.0\n\n\n3\n2015\n1\n1\n...\n188.0\n1535.0\n605.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n389365\n2015\n12\n31\n...\n291.0\n2345.0\n400.0\n\n\n389366\n2015\n12\n31\n...\n132.0\n954.0\n225.0\n\n\n389367\n2015\n12\n31\n...\n198.0\n1744.0\n544.0\n\n\n389368\n2015\n12\n31\n...\n272.0\n2611.0\n753.0\n\n\n\n\n389369 rows × 12 columns\n\n\n\nThis function return a new DataFrame by default. Use inplace=True to modify the DataFrame directly or assigned returned DataFrame to the same name: flights = flights.drop(columns = ['DIST_MILES', 'SPEED_MPH']).\nTo create a copy of the table you can use .copy():\n\nflights_new = flights.copy()\n\nNote that copy() makes a deep copy by default (deep=True). The new DataFrame has its own data and index and changes to flights_new won’t affect flights, and vice versa.\nflights_shallow = flights.copy(deep=False) creates a shallow copy - new DataFrame object that shares the same underlying data buffers.\nIn contrast, flights_new = flights makes no copy at all; both names point to the same DataFrame, so any modification through either name affects the same data.\nWe recommend to always use .copy() to avoid mistakes.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#summary",
    "href": "topic02_Data_Wrangling/script/script_02.html#summary",
    "title": "2  Data Wrangling",
    "section": "2.9 Summary",
    "text": "2.9 Summary\nBy now, you should be able to answer the following questions:\n\nWhat is a DataFrame in pandas and how to load one?\nHow to subset by rows or columns in pandas?\nHow to add columns?\nHow to perform different operations with one or multiple columns?",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic02_Data_Wrangling/script/script_02.html#pandas-resources",
    "href": "topic02_Data_Wrangling/script/script_02.html#pandas-resources",
    "title": "2  Data Wrangling",
    "section": "2.10 Pandas resources",
    "text": "2.10 Pandas resources\n\nPandas Documentation: https://pandas.pydata.org/docs/index.html\n10 minutes to pandas: https://pandas.pydata.org/docs/user_guide/10min.html\nDataCamp course about pandas: https://www.datacamp.com/tutorial/pandas",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html",
    "href": "topic04_Plotting-I/script/script_04.html",
    "title": "3  Low dimensional visualizations",
    "section": "",
    "text": "4 Low dimensional visualizations",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#why-plotting",
    "href": "topic04_Plotting-I/script/script_04.html#why-plotting",
    "title": "3  Low dimensional visualizations",
    "section": "4.1 Why plotting?",
    "text": "4.1 Why plotting?\nPlotting is crucial to data science because:\n\nIt facilitates making new observations by discovering associations or patterns in the data (the initial step of the scientific method 1). The human brain is particularly good at detecting patterns in images, which is what we evolved for. Visual display, over staring at tables of numbers, is very effective.\nIt facilitates communicating findings.\nOnly relying on summary statistics (mean, correlation, etc.) is dangerous. Summary statistics reduce data to a single number, therefore carry much less information than 2D representations. Section 4.1.1 provides examples.\nIt helps debugging either the code by visually checking whether particular operations performed as expected on the data, or by identifying “bugs in the data” such as wrong entries or outliers. Section 4.1.2 provides an example.\n\n\n4.1.1 Plotting versus summary statistics\nWhat do those 13 datasets have in common?\n\n\nAll those plots, including the infamous datasaurus share the same following statistics:\n\nX mean: 52.26\nY mean: 47.83\nX standard deviation: 16.76\nY standard deviation: 29.93\nPearson correlation: -0.06\n\nWhen only looking at the statistics, we would have probably wrongly assumed that the datasets were identical. This example highlights why it is important to visualize data and not just rely on summary statistics. See https://github.com/lockedata/datasauRus or Anscombe’s quartet https://en.wikipedia.org/wiki/Anscombe%27s_quartet for more examples.\n\n\n4.1.2 Plotting helps finding bugs in the data\nConsider the following vector height containing (hypothetical) height measurements for 500 adults:\n\nheight_df.head()\n\n\n\n\n\n\n\n\nheight\n\n\n\n\n0\n1.81\n\n\n1\n1.75\n\n\n2\n1.77\n\n\n3\n1.83\n\n\n4\n1.81\n\n\n\n\n\n\n\nCalculating the mean height returns the following output:\n\nheight_df['height'].mean()\n\nnp.float64(2.05558)\n\n\nThere is something obviously wrong. We can plot the data to investigate.\n\n(ggplot(height_df, aes('height')) + geom_histogram(bins=50) + mytheme)\n\n\n\n\n\n\n\n\nThere is an outlier (height=165). One particular value has probably been entered in centimeters rather than meters. As a result, the mean is inflated.\nA quick way to fix our dataset is to remove the outlier, for instance with:\n\nheight_df = height_df[height_df['height'] &lt; 3]\n\nNow our plotted data seems more realistic and the mean height makes sense.\n\nheight_df['height'].mean()\n\nnp.float64(1.7290380761523045)\n\n\n\n(ggplot(height_df, aes('height')) + geom_histogram() + mytheme)\n\n/Users/maria/Projects/PhD/Dataviz/pydataviz/pydataviz/.dataviz/lib/python3.13/site-packages/plotnine/stats/stat_bin.py:112: PlotnineWarning: 'stat_bin()' using 'bins = 16'. Pick better value with 'binwidth'.\n\n\n\n\n\n\n\n\n\nWhile developing analysis scripts, we recommend to frequently visualize the data to make sure no mistake in the input or during the processing occurred.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#grammar-of-graphics",
    "href": "topic04_Plotting-I/script/script_04.html#grammar-of-graphics",
    "title": "3  Low dimensional visualizations",
    "section": "4.2 Grammar of graphics",
    "text": "4.2 Grammar of graphics\nThe grammar of graphics is a visualization theory developed by Leland Wilkinson in 1999. It has influenced the development of graphics and visualization libraries alike. It is based on 3 key principles:\n\nSeparation of data from aesthetics (e.g. x and y-axis, color-coding)\nDefinition of common plot/chart elements (e.g. scatter plots, box-plots, etc.)\nComposition of these common elements (one can combine elements as layers)\n\nThe library plotnine is a Python implementation inspired by ggplot2 and follows the grammar of graphics closely.\nHere is a sophisticated motivating example. The plot shows the relationship between per-capita gross domestic product (GDP) and life expectancy at birth for the years 1977 and 2007 from the dataset gapminder:\n\ngapminder = pd.read_csv(\"https://raw.github.com/jennybc/gapminder/refs/heads/main/inst/extdata/gapminder.tsv\", sep=\"\\t\")\ngm_df = gapminder[gapminder['year'].isin([1977, 2007])]\n(\n    ggplot(gm_df, aes(x='gdpPercap', y='lifeExp', color='continent', size='pop'))\n    + geom_point()\n    + scale_x_log10()\n    + facet_grid('~year')\n    + labs(x='per-capita GDP', y='Life expectancy at birth', size = 'Population')\n    + mytheme  + theme(figure_size=(6,3))\n)\n\n\n\n\n\n\n\nFigure 4.1\n\n\n\n\n\nWe may, for instance, use such visualization to find differences in the life expectancy of each country and each continent.\nThe following section shows how to create such a sophisticated plot step by step.\n\n4.2.1 Components of the layered grammar\nGrammar of graphics composes plots by combining layers. The major layers are:\n\nAlways used:\nData: a pandas.DataFrame where columns correspond to variables\nAesthetics: mapping of data to visual characteristics - what we will see on the plot (aes) — position (x,y), color, size, shape, transparency\nGeometric objects: geometric representation defining the type of the plot data (geom_) — points, lines, boxplots, …\nOften used:\nScales: for each aesthetic, describes how a visual characteristic is converted to display values (scale_) — log scales, color scales, size scales, …\nFacets: describes how data is split into subsets and displayed as multiple sub graphs (facet_)\nUseful, but with care:\nStats: statistical transformations that typically summarize data (stat) — counts, means, medians, regression lines, …\nDomain-specific usage:\nCoordinate system: describes 2D space that data is projected onto (coord_) — Cartesian coordinates, polar coordinates, map projections, …\n\n\n\n4.2.2 Defining the data and layers\nTo demonstrate the application of grammar of graphics, we will build step by step the gapminder figure Figure 4.1. First, we have a look at the first lines of the dataset:\n\ngm_df[['country','continent','gdpPercap','lifeExp','year']].head()\n\n\n\n\n\n\n\n\ncountry\ncontinent\ngdpPercap\nlifeExp\nyear\n\n\n\n\n5\nAfghanistan\nAsia\n786.113360\n38.438\n1977\n\n\n11\nAfghanistan\nAsia\n974.580338\n43.828\n2007\n\n\n17\nAlbania\nEurope\n3533.003910\n68.930\n1977\n\n\n23\nAlbania\nEurope\n5937.029526\n76.423\n2007\n\n\n29\nAlgeria\nAfrica\n4910.416756\n58.014\n1977\n\n\n\n\n\n\n\nFor starting with the visualization we initiate a ggplot object which generates a plot with background:\n\n(ggplot(gm_df))\n\n\n\n\n\n\n\n\nNext, we can define the data to be plotted, which needs to be a pandas.DataFrame and the aes() function. This aes() function defines which columns map to x and y coordinates and if they should be colored or have different shapes and sizes based on the values in a different column. These elements are called “aesthetic” elements.\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp')))\n\n\n\n\n\n\n\n\nWe want to visualize the data with a simple scatter plot. In a scatter plot, the values of two variables are plotted along two axes. Each pair of values is represented as a point. We combine the function geom_point() to create a scatter plot:\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp'))\n  + geom_point())\n\n\n\n\n\n\n\n\nOne of the advantages of plotting with grammar-of-graphics libraries is that the plot object can be stored and modified. For example:\n\np = ggplot(gm_df, aes(x='gdpPercap', y='lifeExp')) + geom_point()\nprint(type(p))\n\n&lt;class 'plotnine.ggplot.ggplot'&gt;\n\n\n\n\n4.2.3 Mapping of aesthetics\n\nMapping of color, shape and size\nWe can easily map variables to different colors, sizes or shapes depending on the value of the specified variable. To assign each point to its corresponding continent, we can define the variable continent as the color attribute in aes():\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp', color='continent')) + geom_point())\n\n\n\n\n\n\n\n\nInstead of color, we can also use different shapes for characterizing categories (note: some backends limit number of shapes):\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp', shape='continent')) + geom_point())\n\n\n\n\n\n\n\n\nAdditionally, we distinguish the population of each country by giving a size to the points in the scatter plot:\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp', color='continent', size='pop')) + geom_point())\n\n\n\n\n\n\n\n\n\n\nGlobal versus individual mapping\nMapping of aesthetics in aes() can be done globally or at individual layers. Global mapping is inherited by default to all geom layers, while mapping at individual layers is only recognized at that layer. Example:\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp')) + geom_point(aes(color='continent', size='pop')))\n\n\n\n\n\n\n\n\nNote that individual layer mapping cannot be recognized by other layers. For instance, we can add another layer for smoothing with stat_smooth().\n\n# this doesn't work as stat_smooth didn't know aes(x , y)\n(ggplot(gm_df) +\n  geom_point(aes(x='gdpPercap', y='lifeExp')) +\n  stat_smooth(color='blue'))\n\n\n# this works but is redundant\n(ggplot(gm_df) +\n  geom_point(aes(x='gdpPercap', y='lifeExp')) +\n  stat_smooth(aes(x='gdpPercap', y='lifeExp'), color='blue'))\n\n/Users/maria/Projects/PhD/Dataviz/pydataviz/pydataviz/.dataviz/lib/python3.13/site-packages/plotnine/stats/smoothers.py:345: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n\n\n\n\n\n# the common aes(x, y) shared by all the layers can be put in the ggplot()\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp')) +\n  geom_point() +\n  stat_smooth(color='blue'))\n\n/Users/maria/Projects/PhD/Dataviz/pydataviz/pydataviz/.dataviz/lib/python3.13/site-packages/plotnine/stats/smoothers.py:345: PlotnineWarning: Confidence intervals are not yet implemented for lowess smoothings.\n\n\n\n\n\n\n\n\n\n\n\n\n4.2.4 Facets, axes and labels\nFor comparing the data from different years, we can add a facet with facet_wrap():\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp', color='continent', size='pop'))\n   + geom_point() \n   + facet_wrap('~year')\n   + theme(figure_size=(6,3)))\n\n\n\n\n\n\n\n\nFor a better visualization of the data points, we can consider log scaling (detailed in section Section 4.3.3.1.2). Finally, we can adapt the axes labels of the plot with labs() and define a theme of our plot:\n\nmysize = 9\nmytheme = theme(\n    axis_title = element_text(size=mysize), \n    axis_text = element_text(size=mysize),\n    legend_title = element_text(size=mysize),\n    legend_text = element_text(size=mysize),\n    ) + theme_minimal(base_size=mysize)\n\n(ggplot(gm_df, aes(x='gdpPercap', y='lifeExp'))\n    + geom_point(aes(color='continent', size='pop'))\n    + facet_grid('~year')\n    + scale_x_log10()\n    + labs(x='Per-capita GDP', y='Life expectancy at birth', size='Population')\n    + mytheme + theme(figure_size=(6,3)))",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#different-types-of-one--and-two-dimensional-plots",
    "href": "topic04_Plotting-I/script/script_04.html#different-types-of-one--and-two-dimensional-plots",
    "title": "3  Low dimensional visualizations",
    "section": "4.3 Different types of one- and two-dimensional plots",
    "text": "4.3 Different types of one- and two-dimensional plots\nIn the previous examples, we had a look at scatter plots which are suitable for plotting the relationship between two continuous variables. However, there are many more types of plots (e.g. histograms, boxplots) which can be used for plotting in different scenarios. Mainly, we distinguish between plotting one or two variables and whether the variables are continuous or discrete.\n\n4.3.1 Plots for one single continuous variable\n\nHistograms\nA histogram represents the frequencies of values of a variable bucketed into ranges or bins. It takes as input numeric variables only. The height of a bar for a given range in a histogram represents the number of values present in that bin.\nWe will make use of a dataset collecting Human Development Index (HDI from http://hdr.undp.org/) and Corruption Perception Index (CPI from http://www.transparency.org/) of various countries. We first load these data into a new data table ind and have a first look at the table:\n\nind = pd.read_csv('../../extdata/CPI_HDI.csv').drop(columns=['Unnamed: 0'])\nind.head()\n\n\n\n\n\n\n\n\ncountry\nwbcode\nCPI\nHDI\nregion\n\n\n\n\n0\nAfghanistan\nAFG\n12\n0.465\nAsia Pacific\n\n\n1\nAlbania\nALB\n33\n0.733\nEast EU Cemt Asia\n\n\n2\nAlgeria\nDZA\n36\n0.736\nMENA\n\n\n3\nAngola\nAGO\n19\n0.532\nSSA\n\n\n4\nArgentina\nARG\n34\n0.836\nAmericas\n\n\n\n\n\n\n\n\n(ggplot(ind, aes('HDI')) + geom_histogram() + mytheme)\n\n/Users/maria/Projects/PhD/Dataviz/pydataviz/pydataviz/.dataviz/lib/python3.13/site-packages/plotnine/stats/stat_bin.py:112: PlotnineWarning: 'stat_bin()' using 'bins = 6'. Pick better value with 'binwidth'.\n\n\n\n\n\n\n\n\n\nWe can change the number of desired bins 2 in the bins argument of the geom_histogram() function:\n\n(ggplot(ind, aes('HDI')) + geom_histogram(bins=10) + mytheme)\n\n\n\n\n\n\n\n\n\n\nDensity plots\nIn some situations, histograms are not the best choice to investigate the distribution of a variable due to discretization effects during the binning process. A variation of histograms is given by density plots. They are used to represent the distribution of a numeric variable. These distribution plots are typically obtained by kernel density estimation to smoothen out the noise. Thus, the plots are smooth across bins and are not affected by the number of bins, which helps create a more defined distribution shape.\nAs an example, we can visualize the distribution of the Human Development Index (HDI) in the ind dataset by means of a density plot with geom_density():\n\n(ggplot(ind, aes('HDI')) + geom_density() + mytheme)\n\n\n\n\n\n\n\n\nThe bw argument of the geom_density() function allows to tweak the bandwidth of a density plot manually. The default option is a bandwidth rule, which is usually a good choice.\nSetting a small bandwidth on the previous plot has a huge impact on the plot:\n\n(ggplot(ind, aes('HDI')) + geom_density(bw=0.01) + ggtitle('Small bandwidth') + mytheme)\n\n\n\n\n\n\n\n\nSetting a large bandwidth has also a huge impact on the plot:\n\n(ggplot(ind, aes('HDI')) + geom_density(bw=1) + ggtitle('Large bandwidth') + mytheme)\n\n\n\n\n\n\n\n\nThus, we should be careful when changing the bandwidth, since we can get a wrong impression from the distribution of a continuous variable.\n\n\nBoxplots\nBoxplots can give a good graphical insight into the distribution of the data. They show the median, quartiles, and how far the extreme values are from most of the data.\nFour values are essential for constructing a boxplot:\n\nthe median\nthe first quartile (Q1)\nthe third quartile (Q3)\nthe interquartile range (IQR): the difference between Q3 and Q1\n\nSee http://web.pdx.edu/~stipakb/download/PA551/boxplot.html for a good illustration.\nEvery boxplot has lines at Q1, the median, and Q3, which together build the box of the boxplot. The other major feature of a boxplot is its whiskers. The whiskers are determined with the help of the IQR. Data points outside of 1.5 x IQR is called an outlier. We then draw lines at the smallest and largest point within this subset (Q1 - 1.5 × IQR to Q3 + 1.5 × IQR) from the dataset. These lines define our whiskers which reach the most extreme data point within \\(\\pm  1.5\\times IQR\\).\nIt is possible to not show the outliers in boxplots. However, we strongly recommend keeping them. Outliers can reveal interesting data points (discoveries “out of the box”) or bugs in data preprocessing.\nFor instance, we can plot the distribution of a variable x with a histogram and visualize the corresponding boxplot:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoxplots are particularly suited for plotting non-Gaussian symmetric and non-symmetric data and for plotting exponentially distributed data. However, boxplots are not well suited for bimodal data, since they only show one mode (the median). In the following example, we see a bimodal distribution in the histogram and the corresponding boxplot, which does not properly represent the distribution of the data.\n\n\n\n\n\n\n\n\n\n\n\n/Users/maria/Projects/PhD/Dataviz/pydataviz/pydataviz/.dataviz/lib/python3.13/site-packages/plotnine/stats/stat_bin.py:112: PlotnineWarning: 'stat_bin()' using 'bins = 6'. Pick better value with 'binwidth'.\n\n\n\n\n\n\n\n\n\nBoxplots are also not suited for categorical data and discrete data with very few values, for which bar plots are preferred (Section 4.3.2.1).\n\n\n\n4.3.2 Plots for two variables: one continuous, one discrete\n\nBarplots\nBarplots are often used to highlight individual quantitative values per category. Bars are visual heavyweights compared to dots and lines. In a barplot, we can combine two attributes of 2-D location and line length to encode quantitative values. In this manner, we can focus the attention primarily on individual values and support the comparison of one to another.\nFor creating a barplot with plotnine we can use the function geom_bar(). In the next example, we visualize the number of countries (defined in the y axis) per continent (defined in the x axis).\n\ncountries_dt = pd.DataFrame({\n    'Continent': [\"North America\", \"South America\", \"Africa\", \"Asia\", \"Europe\", \"Oceania\"],\n    'Number_countries': [23, 12, 54, 49, 50, 16]\n})\n\n(ggplot(countries_dt, aes('Continent', 'Number_countries')) + geom_bar(stat='identity', width=0.7)\n    + mytheme + theme(figure_size=(6,3)))\n\n\n\n\n\n\n\n\n\n\nBarplots with errorbars\nVisualizing uncertainty is important, otherwise, barplots with bars as a result of an aggregation can be misleading. One way to visualize uncertainty is with error bars.\nAs error bars, we can consider the standard deviation (SD) and the standard error of the mean (SEM). SD and SEM are related yet different concepts. On the one hand, SD indicates the variation of quantity in the sample. On the other hand, SEM represents how well the mean is estimated.\nThe central limit theorem implies that \\(SEM = SD / \\sqrt{n}\\) , where \\(n\\) is the sample size, i.e. the number of observations. With large \\(n\\), SEM tends to 0, i.e. our uncertainty about the distibution’s expected value decreases with larger samples sizes. In contrast, SD converges with large sample size to the distribution’s standard deviation.\nIn the following example (Figure Figure 4.2), we plot the average highway miles per gallon hwy per vehicle class class including error bars computed as the average plus/minus standard deviation of hwy. Because of the various possibilities of error bars used in the literature, it is recommended to always specify in the legend of a figure what the error bars represent.\n\nfrom plotnine.data import mpg\nmpg_df = mpg.copy()\nsummary = mpg_df.groupby('class').agg(mean_hwy=('hwy', 'mean'), sd_hwy=('hwy', 'std')).reset_index()\nsummary['ymax'] = summary['mean_hwy'] + summary['sd_hwy']\nsummary['ymin'] = summary['mean_hwy'] - summary['sd_hwy']\n\n(ggplot(summary, aes('class', 'mean_hwy', ymax='ymax', ymin='ymin'))\n    + geom_bar(stat='identity')\n    + geom_errorbar(width=0.3)\n    + mytheme + theme(figure_size=(6,3)))\n\n\n\n\n\n\n\nFigure 4.2: Mean +/- standard deviation of highway miles per gallon per car class.\n\n\n\n\n\n\n\nBoxplots by category\nAs illustrated before, boxplots are well suited for plotting one continuous variable. However, we can also use boxplots to show distributions of continuous variables with respect to some categories. This can be particularly interesting for comparing the different distributions of each category.\nFor instance, we want to visualize the highway miles per gallon hwy for every one of the 7 vehicle classes (compact, SUV, minivan, etc.). For this, we define the categorical class variable on the x axis and the continuous variable hwy on the y axis.\n\n(ggplot(mpg_df, aes('class', 'hwy')) + geom_boxplot() + mytheme + theme(figure_size=(6,3)))\n\n\n\n\n\n\n\n\nOne can also use geom_jitter(), which arbitrarily adds some randomness to the x-position of the data points within each box in order to separate them visually.\n\np = ggplot(mpg_df, aes('class', 'hwy')) + geom_boxplot() + mytheme + theme(figure_size=(6,3))\n(p + geom_jitter(width=0.2))\n\n\n\n\n\n\n\n\n\n\nViolin plots\nA violin plot is an alternative to the boxplot for visualizing one continuous variable (grouped by categories). An advantage of the violin plot over the boxplot is that it also shows the entire distribution of the data. This can be particularly interesting when dealing with multimodal data.\nFor a direct comparison, we show a violin plot for the hwy grouped by class as before with the help of the function geom_violin():\n\n(ggplot(mpg_df, aes('class', 'hwy')) + geom_violin() + mytheme + theme(figure_size=(6,3)))\n\n\n\n\n\n\n\n\n\n\n\n4.3.3 Plots for two continuous variables\n\nScatter plots\nScatter plots are a useful plot type for easily visualizing the relationship between two continuous variables. Here, dots are used to represent pairs of values corresponding to the two considered variables. The position of each dot on the horizontal (x) and vertical (y) axis indicates values for an individual data point.\nIn the next example, we analyze the relationship between the engine displacement in liters displ and the highway miles per gallon hwy from the mpg dataset:\n\n(ggplot(mpg_df, aes('displ', 'hwy')) + geom_point() + mytheme)\n\n\n\n\n\n\n\n\nWe can modify the previous plot by coloring the points depending on the vehicle class:\n\n(ggplot(mpg_df, aes('displ', 'hwy', color='class')) + geom_point() + mytheme)\n\n\n\n\n\n\n\n\nSometimes, too many colors can be hard to distinguish. In such cases, we can use facet to separate them into different plots:\n\n(ggplot(mpg_df, aes('displ', 'hwy')) + geom_point() + facet_wrap('~class')\n   + mytheme + theme(figure_size=(6,4)))\n\n\n\n\n\n\n\n\n\nText labeling\nFor labeling the individual points in a scatter plot, plotnine offers the function geom_text(). However, these labels tend to overlap. To avoid this, we can use the library ggrepel which offers a better text labeling through the function geom_text_repel().\nWe first show the output of the classic text labeling with geom_text() for a random subset of 40 observations of the dataset mpg. Here we plot the engine displacement in liters displ vs. the highway miles per gallon hwy and label by manufacturer:\n\nmpg_subset = mpg_df.sample(n=30, random_state=12)\n(ggplot(mpg_subset, aes('displ', 'hwy', label='manufacturer')) + geom_point() + geom_text() + mytheme)\n\n\n\n\n\n\n\n\n\n\nLog scaling\nWe consider another example where we want to plot the weights of the brain and body of different animals using the dataset Animals. This is what we obtain after creating a scatterplot.\n\nanimals_df = pd.read_csv(\"https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/MASS/Animals.csv\")\n(ggplot(animals_df, aes('body', 'brain')) + geom_point() + mytheme)\n\n\n\n\n\n\n\n\nWe can clearly see that there are a few points which are notably larger than most of the points. This makes it harder to interpret the relationships between most of these points. In such cases, we can consider logarithmic transformations and/or scaling. More precisely, a first idea would be to manually transform the values into a logarithmic space and plot the transformed values instead of the original values:\n\nanimals_df[\"log_body\"] = np.log10(animals_df[\"body\"])\nanimals_df[\"log_brain\"] = np.log10(animals_df[\"brain\"])\n(ggplot(animals_df, aes('log_body', 'log_brain')) + geom_point() + mytheme)\n\n\n\n\n\n\n\n\nAlternatively, plotnine offers to simply scale the data without the need to transform. This can be done with the help of the functions scale_x_log10() and scale_y_log10() which allow appropriate scaling and labeling of the axes:\n\n(ggplot(animals_df, aes('body', 'brain')) + geom_point() + scale_x_log10() + scale_y_log10() + mytheme)",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#d-density-plots",
    "href": "topic04_Plotting-I/script/script_04.html#d-density-plots",
    "title": "3  Low dimensional visualizations",
    "section": "4.4 2D-Density plots",
    "text": "4.4 2D-Density plots\nUsing scatterplots can become problematic when dealing with a huge number of points. This is due to the fact that points may overlap and we cannot clearly see how many points are at a certain position. In such cases, a 2D density plot is particularly well suited. This plot counts the number of observations within a particular area of the 2D space.\nThe function geom_bin_2d() is creates 2D density plots in python:\n\nx = np.random.randn(10000)\ny = x + np.random.randn(10000)\ndf = pd.DataFrame({'x': x, 'y': y})\n(ggplot(df, aes('x', 'y')) + geom_bin_2d() + mytheme)\n\n\n\n\n\n\n\n\n\nLine plots\nA line plot can be considered for connecting a series of individual data points or to display the trend of a series of data points. This can be particularly useful to show the shape of data as it flows and changes from point to point. We can also show the strength of the movement of values up and down through time.\nAs an example we show the connection between the individual datapoints of unemployment rate over the years:\n\n(ggplot(economics, aes(x='date', y='unemploy_pop_ratio'))\n  + geom_line()\n  + mytheme)",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#further-plots-for-low-dimensional-data",
    "href": "topic04_Plotting-I/script/script_04.html#further-plots-for-low-dimensional-data",
    "title": "3  Low dimensional visualizations",
    "section": "4.5 Further plots for low dimensional data",
    "text": "4.5 Further plots for low dimensional data\n\n4.5.1 Plot matrix\nA plot matrix is useful for exploring the distributions and correlations of a few variables in a matrix-like representation. Here, for each pair of considered variables, a scatterplot is shown. Moreover, a density plot is created for every single variable (diagonal).\nWe can use the function sns.pairplot() from the library seaborn for constructing plot matrices:\n\nimport seaborn as sns\nmpg = sns.load_dataset('mpg')\ncolumns_to_plot = ['displacement', 'cylinders', 'mpg', 'horsepower']\nsns.pairplot(mpg, vars=columns_to_plot, kind='scatter', diag_kind='kde', aspect=2, size=1)\n\n/Users/maria/Projects/PhD/Dataviz/pydataviz/pydataviz/.dataviz/lib/python3.13/site-packages/seaborn/axisgrid.py:2100: UserWarning: The `size` parameter has been renamed to `height`; please update your code.\n\n\n\n\n\n\n\n\n\nThis plot is recommended and suited for a handful of variables but does not scale up to much more variables.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#summary",
    "href": "topic04_Plotting-I/script/script_04.html#summary",
    "title": "3  Low dimensional visualizations",
    "section": "4.6 Summary",
    "text": "4.6 Summary\nThis chapter covered the basics of the grammar of graphics and plotnine to plot low dimensional data. We introduced the different types of plots such as histograms, boxplots or barplots and discussed when to use which plot.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#resources",
    "href": "topic04_Plotting-I/script/script_04.html#resources",
    "title": "3  Low dimensional visualizations",
    "section": "4.7 Resources",
    "text": "4.7 Resources\n\nThe ggplot book: https://ggplot2-book.org/\nThe plotnine guide: https://plotnine.org/reference/\nUdacity’s Data Visualization and D3.js\n\nhttps://www.udacity.com/courses/all\n\nGraphics principles\n\nhttps://onlinelibrary.wiley.com/doi/full/10.1002/pst.1912\nhttps://graphicsprinciples.github.io/",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  },
  {
    "objectID": "topic04_Plotting-I/script/script_04.html#footnotes",
    "href": "topic04_Plotting-I/script/script_04.html#footnotes",
    "title": "3  Low dimensional visualizations",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Scientific_method↩︎\nThere does not seem to be any widely accepted heuristics to choose the number of bins of a histogram based on data. We recommend trying different values if the default seems to be suboptimal.↩︎",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Low dimensional visualizations</span>"
    ]
  }
]