[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Analysis and Visualization in Python",
    "section": "",
    "text": "Data Analysis and Visualization in Python (IN2339)\nThis is the lecture script of the module Data Analysis and Visualization in Python (IN2339).\n\n\nNote — work in progress This book is an adaptation to the programming language Python of the original script Data Analysis and Visualization in R and is still under development. Chapters will be added in order. For any content not yet implemented, please refer to the original. Theoretical concepts are identical, only the programming language is changed.\n\n\nThis work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)\n\n\nAcknowledgments\nThis script has been first put together in the winter semester 2020/2021 by Felix Brechtmann, Alexander Karollus, Daniela Klaproth-Andrade, Pedro Silva, and Julien Gagneur with help from Xueqi Cao, Laura Martens, Ines Scheller, Vangelis Theodorakis, and Vicente Yépez.\nWe leveraged work from colleagues who helped creating lecture slides since 2017: Žiga Avsec, Ines Assum, Daniel Bader, Jun Cheng, Bašak Eraslan, Mathias Heinig, Jan Krumsieck, Christian Mertes, and Georg Stricker.\nThe script has been adapted from R to Python in the winter semester 2025/2026 by Xavi Hernandez Alias, Eva Holtkamp, Shubhankar Londhe, Johann Promeuschel, Maria Ryabtseva and Anna Starovoit\n\n\nPrerequisites\nBasics in probabilities are required. Chapters 13-15 (“Introduction to Statistics with R”, “Probability” and “Random variables”) of the Book “Introduction to Data Science” https://rafalab.github.io/dsbook/ make a good refresher. Make sure all concepts are familiar to you. Check your knowledge by trying the exercises.\n\n\nDatasets\nDatasets used in this script are available to download as a compressed file here.\n\n\nFeedback\nFor improvement suggestions, reporting errors and typos, please use the online document here.",
    "crumbs": [
      "Data Analysis and Visualization in Python (IN2339)"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html",
    "href": "topic00_Introduction/script/script_00.html",
    "title": "Introduction",
    "section": "",
    "text": "Data Science: What and why?\nData science is an interdisciplinary field about processes and systems to extract knowledge or insights from data. The goals of Data Science include discovering new phenomena or trends from data, enabling decisions based on facts derived from data, and communicating findings from data. It is a continuation of some of the data analysis fields such as statistics, data mining, and predictive analytics.\nData Science is at the heart of the scientific method, which starts with making data-driven observations to formulate testable hypotheses. It furthermore comes into play to visualize and assess experimental results. Data science skills are therefore necessary to any field of scientific research. Data science is the main tools of epidemiology, the study of health and disease in populations, which largely relies on observational data. Moreover data science is important in the industry, to understand operational process, and in business analytics, to understand a particular market. Hence, with the rise of big data in all areas of society, data science skills are some of the most demanded skills on the job market. Last, but not least, in an era of fake news, data science skills are important for citizens of modern societies.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#what-you-will-learn-and-not-learn",
    "href": "topic00_Introduction/script/script_00.html#what-you-will-learn-and-not-learn",
    "title": "Introduction",
    "section": "What you will learn and not learn",
    "text": "What you will learn and not learn\nThe goal of this course is to provide you with general analytic techniques to extract knowledge, patterns, and connections between samples and variables, and how to communicate these in an intuitive and clear way.\nThis course focuses on front-end data science. This means, it teaches practical skills to analyse data. We will focus on tidy data, visualizations, and data manipulation in Python. To only then dive into the math required to understand and interpret analysis results.\nThis course does not teach back-end data science, i.e. it does not teach how to develop your own statistical or machine learning models, nor how to develop scalable data processing software.\nOther courses offered by the faculty of Informatics cover data science back-end skills.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#the-python-language",
    "href": "topic00_Introduction/script/script_00.html#the-python-language",
    "title": "Introduction",
    "section": "The Python language",
    "text": "The Python language\nPython is a versatile high-level programming language used intensively for data analytics and machine learning. It is a great language for front-end data science, i.e. to rapidly manipulate, visualize and come to raising interesting hypotheses.\nBut thanks to its versatility, even complicated back-end software, such as deep-learning models, can be designed and maintained with Python. There is always a trade-off between abstraction from the computer language — i.e., legibility for humans — and the efficiency of programming languages. As Python is a high-level language, its purpose is to reduce time spent coding and to maximize users’ time looking at and thinking about the data, rather than minimizing the computer’s running time. Because researchers and data analysts often need to make quick, effective analyses and usually don’t have time to optimize low-level code, Python offers an ideal compromise between these two worlds.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#course-overview",
    "href": "topic00_Introduction/script/script_00.html#course-overview",
    "title": "Introduction",
    "section": "Course overview",
    "text": "Course overview\nThe lecture is structured into three main parts covering the major steps of data analysis:\n\nGet the data: After basic introduction to Python, learn how to fetch and manipulate real-world datasets. How to structure them to most conveniently work with them (tidy data).\nLook at the data: Basic and advanced visualization techniques allows navigating large and complex datasets, identifying interesting signal, and formulating hypotheses. Typical sources of confounding are discussed. Recommendation to present an analysis in compelling fashion are also given.\nConclude: Concepts of hypothesis testing will allow concluding about the statistical robustness of discovered associations. Also, methods from supervised learning will allow to model data and build accurate predictors.\n\nThe chapters of this script corresponds to individual lectures. Appendices provide further technical details as well as Python tricks and tips.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic00_Introduction/script/script_00.html#complementary-reading",
    "href": "topic00_Introduction/script/script_00.html#complementary-reading",
    "title": "Introduction",
    "section": "Complementary reading",
    "text": "Complementary reading\n— TODO —\nThese books offer complementary information to this script:\n\nIntroduction to Data Science, Rafael A. Irizarry [https://rafalab.github.io/dsbook/]\nR for Data Science, Garrett Grolemund and Hadley Wickham [https://r4ds.had.co.nz/]\nStatistical Inference via Data Science, Chester Ismay and Albert Y. Kim [https://moderndive.com/]\nFundamentals of Data Visualization, Claus O. Wilke [https://clauswilke.com/dataviz/]\nAdvanced R, Hadley Wickham [https://adv-r.hadley.nz/]",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html",
    "href": "topic01_Python_Basics/script/script_01.html",
    "title": "1  Python Basics for Data Science",
    "section": "",
    "text": "1.1 Python Environments\nThis chapter provides a quick introduction to the programming language Python and common tools used for data analysis.\nWhile you can run Python from a system terminal, for data analysis, it’s highly recommended to use an Integrated Development Environment (IDE) or a notebook environment. These tools provide features like code completion, debugging, and interactive data exploration.\nPopular choices include:\nThese environments help organize your work by providing panes for writing scripts, an interactive console to execute code, a variable explorer to inspect objects, and areas to view plots and documentation.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#python-environments",
    "href": "topic01_Python_Basics/script/script_01.html#python-environments",
    "title": "1  Python Basics for Data Science",
    "section": "",
    "text": "Google Colaboratory (Colab): A free, cloud-based Jupyter Notebook environment that requires no setup and runs entirely in the browser. It’s an excellent choice for beginners and for collaborative projects, as notebooks can be easily shared like Google Docs. It also provides free access to powerful hardware like GPUs, making it ideal for machine learning and data science tasks.\nJupyter Notebook/JupyterLab: A web-based interactive environment that allows you to create documents containing live code, equations, visualizations, and narrative text. This format is excellent for exploratory data analysis and sharing results. It can be run on your local machine.\nVisual Studio Code (VS Code): A powerful, free code editor with excellent Python and Jupyter Notebook support through its extensions. It combines the features of a traditional IDE with the interactivity of notebooks, making it a great choice for projects that involve both scripting and data exploration.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#first-steps-with-python",
    "href": "topic01_Python_Basics/script/script_01.html#first-steps-with-python",
    "title": "1  Python Basics for Data Science",
    "section": "1.2 First Steps With Python",
    "text": "1.2 First Steps With Python\nThis section is inspired by the book Introduction to Data Science by Rafael Irizarry, adapted for Python.\n\n1.2.1 Objects\nSuppose a high school student asks for help solving several quadratic equations of the form \\(ax^2+bx+c = 0\\). The quadratic formula gives us the solutions:\n\\[\n\\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}\n\\]\nwhich change depending on the values of \\(a\\), \\(b\\), and \\(c\\). A key advantage of programming is that we can define variables and write expressions that solve these equations numerically. If we are asked to solve \\(x^2 + x -1 = 0\\), we first define our variables:\n\nimport numpy as np\n\na = 1\nb = 1\nc = -1\n\nHere, we use = to assign values to variables. This stores the values for later use. Note that Python doesn’t print anything when we make this assignment, which means the objects were defined successfully.\nTo see the value stored in a variable, you can simply type its name in an interactive console or notebook cell, or use the print function:\n\na\n\n1\n\n\nA more explicit way is to use print():\n\nprint(a)\n\n1\n\n\nWe use the term object to describe things stored in Python, such as variables, functions, and more complex data structures.\n\n\n1.2.2 The Namespace\nAs we define objects, we are changing the current namespace. In an interactive environment like Jupyter or IPython, you can see the user-defined variables using the magic command %whos.\n\n%whos\n\nVariable     Type        Data/Info\n----------------------------------\na            int         1\nb            int         1\nc            int         -1\nnp           module      &lt;module 'numpy' from '/Us&lt;...&gt;kages/numpy/__init__.py'&gt;\nojs_define   function    &lt;function ojs_define at 0x10fbf7f60&gt;\n\n\nYou should see a, b, and c. If you try to access a variable that hasn’t been defined, you’ll get an error. For example, typing x will result in a NameError.\nNow that our variables are defined, we can use the quadratic formula to find the solutions: We have to import numpy module to use the sqrt function from it.\n\nimport numpy as np\n\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nprint(solution_1)\nprint(solution_2)\n\n0.6180339887498949\n-1.618033988749895\n\n\n\n\n1.2.3 Functions\nOnce you define variables, the data analysis process can usually be described as a series of functions applied to data. Python has many built-in functions, and countless more are available through external libraries like NumPy, Pandas, and Scikit-learn.\nWe already used print() and np.sqrt(). In general, we use parentheses to call a function. If you type a function’s name without parentheses, you’ll see a reference to the function object itself, not its result. There are many more prebuilt functions and even more can be added through packages. These functions do not appear in the workspace because you did not define them, but they are available for immediate use.\nMost functions take one or more arguments. For example, the np.log() function calculates the natural logarithm of a number:\n\nnp.log(8)\n\nnp.float64(2.0794415416798357)\n\n\nYou can find out what a function does and what arguments it expects by using the built-in help() function or, in an interactive environment, by typing a question mark ? after the function name. The function round() returns the closest integer to a given number.\n\nhelp(round)\n\nThe help page shows that round() can take a second argument ndigits for the precision. Arguments can be passed by position or by name (keyword arguments).\n\nround(3.1415, 2)\n\n3.14\n\n\n\nround(3.1415, ndigits=2)\n\n3.14\n\n\nWhen using keyword arguments, the order doesn’t matter:\n\nround(ndigits=2, number=3.1415)\n\n3.14\n\n\nArithmetic and relational operators (+, -, *, /, ^, ==, &gt;) are also fundamental. Note that in Python, the power operator is **, not ^.\n\n2**3\n\n8\n\n\n\n\n1.2.4 Variable Names\nWe’ve used a, b, and c, but variable names can be more descriptive. Python variable names must start with a letter or underscore, can contain numbers, but cannot contain spaces, and should not be variables that are predefined in Python. For example, don’t name one of your variables list by typing something like list = 2.\nA common convention in Python is to use snake_case for variable names: use meaningful words, all lowercase, separated by underscores.\n\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nFor more advice, we highly recommend studying PEP8 style guide1.\n\n\n1.2.5 Reproducible Analysis\nOne of the most powerful aspects of programming for data science is the ability to create reproducible analyses. This means that your work can be easily repeated, verified, and modified by yourself or others at any point in the future.\n\nWriting Code That Runs Reliably\nWhen working with notebooks or interactive environments, it’s crucial to write code that produces the same results every time it’s executed. Here are some key principles:\nExecute cells in order: Always run your notebook cells from top to bottom in sequential order. Variables and functions defined in earlier cells are needed by later cells. If you jump around or skip cells, you might get unexpected errors or incorrect results.\nAvoid state-dependent operations: Write code so that running a cell multiple times doesn’t break your analysis. For example:\n# BAD: This will give different results each time you run it\nx = 10\nx = x + 1  # If you run this cell twice, x becomes 12, then 13, etc.\nprint(x)\n# GOOD: This always gives the same result\nx = 10\ny = x + 1  # No matter how many times you run this, y is always 11\nprint(y)\n\n\nThe Power of Reusable Code\nTo solve another equation such as \\(3x^2 + 2x -1\\), we can copy and paste the code above and then redefine the variables and recompute the solution:\n\na = 3\nb = 2\nc = -1\n\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nprint(solution_1, solution_2)\n\n0.3333333333333333 -1.0\n\n\nBy creating and saving a script with the code above, we would not need to retype everything each time and, instead, simply change the variable names. Try writing the script above into an editor and notice how easy it is to change the variables and receive an answer.\n\n\n\n1.2.6 Commenting Your Code\nAny line in Python that starts with a hash symbol # is a comment and is not executed. Use comments to explain what your code is doing.\n\n# Code to compute solution to a quadratic equation of the form ax^2 + bx + c\n\n# Define the variables\na = 3 \nb = 2\nc = -1\n\n# Now compute the solution\nsolution_1 = (-b + np.sqrt(b**2 - 4*a*c)) / (2*a)\nsolution_2 = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n\nLonger comments, e.g., documentation, can be wrapped with triple quotes (““” or ’’’).",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#data-types",
    "href": "topic01_Python_Basics/script/script_01.html#data-types",
    "title": "1  Python Basics for Data Science",
    "section": "1.3 Data Types",
    "text": "1.3 Data Types\nVariables in Python can be of different types. For example, we need to distinguish numbers from character strings and tables from simple lists of numbers. The type() function helps determine an object’s type.\n\na = 2\ntype(a)\n\nint\n\n\nTo work efficiently, it’s important to understand Python’s basic data types.\n\n1.3.1 Numbers\nPython can handle integers (int) and floating-point numbers (float):\n\nx = 7\ny = 3.14\nprint(type(x))\nprint(type(y))\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n\n\n\n\n1.3.2 Strings\nText is represented as strings (str), written in quotes:\n\nname = \"Python\"\nprint(type(name))\n\n&lt;class 'str'&gt;\n\n\nYou can combine strings with the + operator:\n\ngreeting = \"Hello \" + name\nprint(greeting)\n\nHello Python\n\n\n\n\n1.3.3 Lists\nA list is an ordered, mutable collection of items, which can be of different types. You access elements using their zero-based index.\nrecord2 = [\"John Doe\", 1234]\nprint(record2[0]) # Access the first element\n\n\n1.3.4 Dictionaries\nA dictionary (dict) stores key-value pairs and is similar to a named list in R.\nrecord = {\n\"name\": \"John Doe\",\n\"student_id\": 1234,\n\"grades\": [95, 82, 91, 97, 93],\n\"final_grade\": \"A\"\n}\nThis dictionary contains a string, an integer, a list of numbers, and another string.\nprint(record)\nprint(type(record))\nYou access elements in a dictionary using their keys in square brackets:\nrecord['student_id']\n\n\n1.3.5 Boolean\nAnother important data type is a boolean (bool), which can only be True or False. Relational operators like &lt;, &gt;, ==, &lt;=, &gt;= produce booleans.\n\nz = 3 == 2\nprint(z)\nprint(type(z))\n\nFalse\n&lt;class 'bool'&gt;\n\n\nHere the == is a relational operator asking if 3 is equal to 2. In Python, if you just use one =, you actually assign a variable, but if you use two == you test for equality.\n\n\n1.3.6 DataFrames\nThe most common way to store a tabular dataset in Python is in a DataFrame, which is the primary data structure in the pandas library. A DataFrame is a two-dimensional table where rows represent observations and columns represent variables.\nAs an example, let’s use the tips dataset, which records restaurant bills and tips, along with information about the server, day, and time. The dataset is available directly through the Seaborn library.\n\nimport pandas as pd\nimport seaborn as sns\n\n# Load the dataset\ntips = sns.load_dataset(\"tips\")\n\nWe can check the object’s type to confirm that it’s a DataFrame:\n\ntype(tips)\n\npandas.core.frame.DataFrame\n\n\nYou can quickly preview the first few rows of the data using the head() method:\n\ntips.head()\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\nTo access one of the columns, use its name in the access operator []\n\ntips['time']\n\n0      Dinner\n1      Dinner\n2      Dinner\n3      Dinner\n4      Dinner\n        ...  \n239    Dinner\n240    Dinner\n241    Dinner\n242    Dinner\n243    Dinner\nName: time, Length: 244, dtype: category\nCategories (2, object): ['Lunch', 'Dinner']\n\n\n\n\n1.3.7 Categorical Data\nIn the tips dataset, several columns (like sex, smoker, day, and time) contain categorical data — values that fall into a small set of distinct groups rather than continuous numbers.\nPandas provides a special category data type that is more memory-efficient and allows you to store an explicit order for these categories.\n\nimport seaborn as sns\nimport pandas as pd\n\n# Load example dataset\ntips = sns.load_dataset('tips')\n\n# Check which columns are categorical\ntips.dtypes\n\ntotal_bill     float64\ntip            float64\nsex           category\nsmoker        category\nday           category\ntime          category\nsize             int64\ndtype: object\n\n\nLet’s look at one categorical column — day (the day of the week the tip was recorded).\n\n# Convert 'day' to category type\ntips['day'] = tips['day'].astype('category')\ntips['day'].dtype\n\nCategoricalDtype(categories=['Thur', 'Fri', 'Sat', 'Sun'], ordered=False, categories_dtype=object)\n\n\nWe can inspect the unique categories using the .cat.categories accessor:\n\ntips['day'].cat.categories\n\nIndex(['Thur', 'Fri', 'Sat', 'Sun'], dtype='object')\n\n\nBy default, categories have no particular order. But we can reorder them to reflect the natural order of the week.\n\n# Define a meaningful order for the days\nordered_days = ['Thur', 'Fri', 'Sat', 'Sun']\n\n# Re-categorize 'day' with an explicit order\ntips['day'] = pd.Categorical(tips['day'],\n                             categories=ordered_days,\n                             ordered=True)\n\n# Check the new order\ntips['day'].cat.categories\n\nIndex(['Thur', 'Fri', 'Sat', 'Sun'], dtype='object')\n\n\n\n\n1.3.8 NumPy Arrays\nNumPy arrays are the fundamental object for numerical computing in Python. They are list-like or grid-like structures where all elements must be of the same type. While pandas DataFrames are great for heterogeneous, labeled data, NumPy arrays are optimized for homogeneous, numerical array operations. Many pandas operations use NumPy arrays under the hood. Vectorized operations on NumPy arrays are significantly faster than native Python loops due to highly optimized implementation.\nWe can create a NumPy array (often called a matrix when it’s 2D) as follows. We’ll need to import the numpy library, conventionally as np. Use the arange function to create an array with sequential numbers; reshape allows you to rearrange multidimensional arrays. For example, here the initially “flat” array will be shaped into a matrix by filling rows from left to right.\n\nimport numpy as np\n\nmat = np.arange(1, 13).reshape(4, 3)\nprint(mat)\n\n[[ 1  2  3]\n [ 4  5  6]\n [ 7  8  9]\n [10 11 12]]\n\n\nYou can check the type of the element stored in the array with dtype.\n\nmat.dtype\n\ndtype('int64')\n\n\nTo convert it to another type, use astype.\n\nmat_float = mat.astype(np.float64)\nmat_float.dtype\n\ndtype('float64')\n\n\nYou can access specific elements using [row, column] indexing, remembering that Python uses 0-based indexing.\n\n# Access element in the second row, third column\nmat[1, 2]\n\nnp.int64(6)\n\n\nTo get the entire second row, use a colon : for the column index:\n\n# Get the second row (index 1)\nmat[1, :]\n\narray([4, 5, 6])\n\n\nSimilarly, to get the entire third column:\n\n# Get the third column (index 2)\nmat[:, 2]\n\narray([ 3,  6,  9, 12])\n\n\nYou can access more than one column or more than one row if you like. This will give you a new matrix. Note, that the limits are specified as interval and the third colun won’t be included.\n\nmat[:, 1:3]\n\narray([[ 2,  3],\n       [ 5,  6],\n       [ 8,  9],\n       [11, 12]])\n\n\nYou can subset both rows and columns:\n\nmat[1:3, 2:4]\n\narray([[6],\n       [9]])\n\n\nIndices in python are “wrapped” around the end of the array, so you can use negative indices. For example, a handy way to get the last row would be: You can subset both rows and columns:\n\nmat[-1]\n\narray([10, 11, 12])\n\n\nIn general, the access operator arr[i:j:k] accepts 3 arguments. This means take every kth element from ith to jth (excluding the end). Omitting i or j means that you want to take the corresponding end of the array.\n\narr = np.arange(10)\narr[1:-1:2]\n\narray([1, 3, 5, 7])\n\n\nWe can convert a NumPy array into a pandas DataFrame:\n\npd.DataFrame(mat, columns=['A', 'B', 'C'])\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n2\n7\n8\n9\n\n\n3\n10\n11\n12\n\n\n\n\n\n\n\n\n\n1.3.9 List And Arrays: References And Copies\nList and arrays variables are just references. Always copy lists and arrays when reassigning! Compare:\n\na = [1, 2, 3]\nb = a\nb[1] = 5\nprint(a)\n\n[1, 5, 3]\n\n\n\na = [1, 2, 3]\nb = a[:]\nb[1] = 5\nprint(a)\n\n[1, 2, 3]\n\n\n\n\n1.3.10 Ranges\nYou can create sequences of numbers using Python’s built-in range() function or NumPy’s np.arange() function, which is more flexible as it allows for non-integer steps.\n\n# Integers from 1 up to (but not including) 11\nlist(range(1, 11))\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\n\n\n# A sequence from 1 to 10 with a step of 2\nlist(range(1, 11, 2))\n\n[1, 3, 5, 7, 9]\n\n\n\n# A sequence with floating point steps using numpy\nnp.arange(1, 10, 0.5)\n\narray([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5, 6. , 6.5, 7. ,\n       7.5, 8. , 8.5, 9. , 9.5])\n\n\n\n\n1.3.11 Non numerical and infinity (nan, inf)\nWhen working with numerical data, certain operations can render some of the cells invalid. Numpy will store np.nan, or np.inf in that case. One of the common examples is division by 0:\n\nnp.int64(1) / np.int64(0)\n\nnp.float64(inf)\n\n\nNumPy uses a highly optimized underlying implementation, so the types there look more complicated, for example int64 instead of just int in native Python. This just means that this type uses 64 bits, whereas in native Python the int type is not restricted.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#numpy-operation",
    "href": "topic01_Python_Basics/script/script_01.html#numpy-operation",
    "title": "1  Python Basics for Data Science",
    "section": "1.4 Numpy operation",
    "text": "1.4 Numpy operation\nNumpy library comes with a handful set of operation that help working with data. Let’s load a murders dataset. This dataset includes gun murder data for US states in 2010.\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv'\nmurders = pd.read_csv(url)\nmurders.head()\n\n\n\n\n\n\n\n\nstate\nabb\nregion\npopulation\ntotal\n\n\n\n\n0\nAlabama\nAL\nSouth\n4779736\n135\n\n\n1\nAlaska\nAK\nWest\n710231\n19\n\n\n2\nArizona\nAZ\nWest\n6392017\n232\n\n\n3\nArkansas\nAR\nSouth\n2915918\n93\n\n\n4\nCalifornia\nCA\nWest\n37253956\n1257\n\n\n\n\n\n\n\nWe will be working with individual columns, converted to vectors. To convert a column to a NumPy array, use the .values attribute after accessing a column. Note: it’s not a function call, so parentheses are not needed.\n\nmurders['state'].values\n\narray(['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California',\n       'Colorado', 'Connecticut', 'Delaware', 'District of Columbia',\n       'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', 'Indiana',\n       'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland',\n       'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi',\n       'Missouri', 'Montana', 'Nebraska', 'Nevada', 'New Hampshire',\n       'New Jersey', 'New Mexico', 'New York', 'North Carolina',\n       'North Dakota', 'Ohio', 'Oklahoma', 'Oregon', 'Pennsylvania',\n       'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee',\n       'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n       'West Virginia', 'Wisconsin', 'Wyoming'], dtype=object)\n\n\n\n1.4.1 np.sort()\nSay we want to sort the vector, use .sort() operation.\n\ntotal = murders['total'].values\ntotal = np.sort(total)\ntotal[:10]\n\narray([ 2,  4,  5,  5,  7,  8, 11, 12, 12, 16])\n\n\n\n\n1.4.2 .max() and .argmax()\nIf we only want the largest value, we can use .max(). To get the index of the largest value, we use .argmax().\n\n# Get the maximum value in the 'total' column\nmax_total = murders['total'].values.max()\nprint(max_total)\n\n# Get the index of the state with the maximum total\ni_max = murders['total'].values.argmax()\n\n# Use that index to verify\nmurders['total'].values[i_max]\n\n# Find out which state has the most murders.\n# Note: the order of entries when getting .values is the same\n\nmurders['state'].values[i_max]\n\n1257\n\n\n'California'\n\n\n\n\n1.4.3 `.sum()``\nCompute a sum of the array by using the sum function.\n\nmurders['total'].values.sum()\n\nnp.int64(9403)\n\n\n\n\n1.4.4 Vectorized Arithmetics\nCalifornia has the most murders, but it also has the largest population. To compare safety, we should look at the murder rate per capita. The powerful vectorized arithmetic capabilities of NumPy make this easy.\n\n\n1.4.5 Operations on a vectors\nArithmetic operations on NumPy arrays are applied element-wise.\n\ninches = np.array([69, 62, 66, 70, 70, 73, 67, 73, 67, 70])\ncentimeters = inches * 2.54\nprint(centimeters)\n\n[175.26 157.48 167.64 177.8  177.8  185.42 170.18 185.42 170.18 177.8 ]\n\n\nSimilarly, we can subtract a single number from every element:\n\ninches - 69\n\narray([ 0, -7, -3,  1,  1,  4, -2,  4, -2,  1])\n\n\n\n\n1.4.6 Operations between two arrays\nIf we perform an operation between two arrays of the same length, the operation is performed element-wise, matching elements by their index.\nThis means that to compute the murder rate per 100,000 people, we can simply divide the total array by the population array and multiply by 100,000.\n\nmurder_rate = murders['total'].values / murders['population'].values * 100_000\n\nNote that the number 100000 in Python is written with an underscore to improve readability; Python supports both options.\nNow we can add this rate as a new column to our DataFrame and sort by it to get a more meaningful ranking of state safety.\n\nmurders['rate'] = murder_rate\nidx_max_rate = murder_rate.argmax()\nmurders['state'].values[idx_max_rate]\n\n'District of Columbia'\n\n\n\n\n1.4.7 Boolean Indexing\nWe can use a logical condition to create a boolean arrays (True/False values). For example, let’s find states with a murder rate less than 0.71.\n\nind = murders['rate'].values &lt; 0.71\nind.dtype\n\ndtype('bool')\n\n\nWe can then use this boolean array inside square brackets [] to filter any other array of the same size, or even the whole dataframem keeping only the rows where the condition is True.\n\nmurders['state'][ind]\n\n11           Hawaii\n15             Iowa\n29    New Hampshire\n34     North Dakota\n45          Vermont\nName: state, dtype: object\n\n\nTo count how many states meet this condition, we can take the sum() of the boolean array, since True is treated as 1 and False as 0.\n\nind.sum()\n\nnp.int64(5)\n\n\n\n\n1.4.8 Logical Operators\nWe can combine multiple logical conditions using the operators & (and), | (or), and ~ (not). For example, to find safe states in the West region (murder rate &lt;= 1):\n# Note the parentheses around each condition, which are required\nis_west = murders['region'].values == 'West'\nis_safe = murders['rate'].values &lt;= 1\n\nind = is_west & is_safe\nmurders['state'][ind]",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#python-programming-basics",
    "href": "topic01_Python_Basics/script/script_01.html#python-programming-basics",
    "title": "1  Python Basics for Data Science",
    "section": "1.5 Python Programming Basics",
    "text": "1.5 Python Programming Basics\nSo far we’ve focused on using Python interactively for data analysis. But Python is also a powerful general-purpose programming language. The key building blocks for writing more complex programs and automating tasks are:\n\nConditionals (if, elif, else): To execute code only if a certain condition is met.\nLoops (for, while): To repeat a block of code multiple times.\nFunctions (def): To bundle code into reusable, named blocks.\n\nThese concepts allow you to build complex data analysis pipelines, create custom tools, and move from simple scripts to sophisticated programs.\n\n1.5.1 A Simple Loop Example\nLet’s see a basic example of a for loop that solves multiple quadratic equations:\n\n# Solve multiple quadratic equations using a loop\nequations = [\n    {\"a\": 1, \"b\": 1, \"c\": -1},    # x^2 + x - 1 = 0\n    {\"a\": 2, \"b\": -3, \"c\": 1},   # 2x^2 - 3x + 1 = 0\n    {\"a\": 1, \"b\": 0, \"c\": -4}    # x^2 - 4 = 0\n]\n\nfor i, eq in enumerate(equations):\n    a, b, c = eq[\"a\"], eq[\"b\"], eq[\"c\"]\n    discriminant = b**2 - 4*a*c\n    \n    if discriminant &gt;= 0:\n        solution_1 = (-b + np.sqrt(discriminant)) / (2*a)\n        solution_2 = (-b - np.sqrt(discriminant)) / (2*a)\n        print(f\"Equation {i+1}: {a}x² + {b}x + {c} = 0\")\n        print(f\"Solutions: x₁ = {solution_1:.3f}, x₂ = {solution_2:.3f}\")\n    else:\n        print(f\"Equation {i+1}: {a}x² + {b}x + {c} = 0\")\n        print(\"No real solutions (complex roots)\")\n    print(\"-\" * 40)\n\nEquation 1: 1x² + 1x + -1 = 0\nSolutions: x₁ = 0.618, x₂ = -1.618\n----------------------------------------\nEquation 2: 2x² + -3x + 1 = 0\nSolutions: x₁ = 1.000, x₂ = 0.500\n----------------------------------------\nEquation 3: 1x² + 0x + -4 = 0\nSolutions: x₁ = 2.000, x₂ = -2.000\n----------------------------------------\n\n\nThis loop demonstrates how we can process multiple datasets or perform repetitive calculations efficiently, rather than copying and pasting the same code multiple times.\nCreate your own functions using def. You can plot simple graphs using plotnine package.\n\nimport pandas as pd\nfrom plotnine import ggplot, aes, geom_line, theme_minimal, theme\n\ndef parabola(n):\n    if n &gt; 0 and type(n) == int:\n        # Create data for the parabola\n        x = np.array(range(-n, n+1))\n        y = x**2\n        \n        # Create a DataFrame for plotnine\n        df = pd.DataFrame({'x': x, 'y': y})\n        \n        # Create the plot using plotnine\n        plot = (ggplot(df, aes(x='x', y='y')) +\n                geom_line() +\n                theme_minimal() +\n                theme(figure_size=(4,3)))\n        \n        return plot\n\nparabola(10)",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#python-is-indentation-sensitive",
    "href": "topic01_Python_Basics/script/script_01.html#python-is-indentation-sensitive",
    "title": "1  Python Basics for Data Science",
    "section": "1.6 Python Is Indentation Sensitive",
    "text": "1.6 Python Is Indentation Sensitive\nIndentation defines block structure (e.g., inside functions, loops, conditionals). Mixing tabs and spaces is discouraged.\n\n# Correct indentation\nif True:\n    print(\"This is indented\")\n    if 1 &lt; 2:\n        print(\"Nested block\")\n\n# Incorrect indentation would raise an IndentationError\n\nThis is indented\nNested block",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#random-generation-examples-from-distributions",
    "href": "topic01_Python_Basics/script/script_01.html#random-generation-examples-from-distributions",
    "title": "1  Python Basics for Data Science",
    "section": "1.7 Random Generation Examples From Distributions",
    "text": "1.7 Random Generation Examples From Distributions\nIn this course, we often use simulated data to explore data visualization and modeling concepts. Generating our own data helps us focus on understanding patterns, relationships, and visualization techniques without worrying about the complexity or limitations of real datasets.\n\n# random integers\nnp.random.seed(0)\nprint('randints:', np.random.randint(low=0, high=10, size=5))\n\n# normal distribution (mean, std)\nprint('normal sample:', np.random.normal(loc=0.0, scale=1.0, size=5))\n\n# uniform\nprint('uniform:', np.random.rand(5))\n\n# choice with probabilities\nprint('choice:', np.random.choice(['red', 'green', 'blue'], size=5,\\\n    p=[0.1, 0.7, 0.2]))\n\nrandints: [5 0 3 3 7]\nnormal sample: [ 1.86755799 -0.97727788  0.95008842 -0.15135721 -0.10321885]\nuniform: [0.79172504 0.52889492 0.56804456 0.92559664 0.07103606]\nchoice: ['red' 'red' 'blue' 'green' 'blue']\n\n\n\nScripts vs. Jupyter Notebooks: Choosing the Right Tool\nWhen working with Python for data analysis, you have two main approaches: Python scripts (.py files) and Jupyter notebooks (.ipynb files). Each has its strengths and is suited for different types of work.\nPython Scripts (.py files): - Best for: Production code, automated pipelines, functions and modules, version control - Advantages: - Clean, linear execution from top to bottom - Easy to version control with Git (plain text format) - Excellent for creating reusable functions and modules - Faster execution and debugging - Better for automated workflows and deployment - Use cases: Data cleaning pipelines, analysis functions, automated reports, web applications\nJupyter Notebooks (.ipynb files): - Best for: Exploratory analysis, prototyping, teaching, presenting results - Advantages: - Interactive development with immediate feedback - Mix code, visualizations, and narrative text in one document - Cell-by-cell execution allows for iterative exploration - Excellent for data visualization and sharing insights - Built-in support for rich media (plots, tables, LaTeX) - Use cases: Data exploration, research documentation, tutorials, presentations",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#best-practices-for-reproducible-analysis",
    "href": "topic01_Python_Basics/script/script_01.html#best-practices-for-reproducible-analysis",
    "title": "1  Python Basics for Data Science",
    "section": "1.8 Best Practices for Reproducible Analysis",
    "text": "1.8 Best Practices for Reproducible Analysis\n\nStart with notebooks for exploration: Use Jupyter notebooks to explore your data, test hypotheses, and prototype your analysis.\nRefactor to scripts for production: Once your analysis is solid, extract reusable functions into Python scripts (.py files) that can be imported and used across projects.\nDocument your workflow: Whether using scripts or notebooks, include clear comments and markdown explanations of your methodology.\nUse version control: Track changes to your analysis over time using Git, especially for script files.\nMake your environment reproducible: Use tools like requirements.txt or conda environment files to specify exact package versions.\n\nThe goal is to create analyses that can be easily understood, modified, and reproduced by anyone (including your future self) who needs to work with your code. These concepts allow you to build complex data analysis pipelines, create custom tools, and move from simple scripts to sophisticated programs.",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "topic01_Python_Basics/script/script_01.html#footnotes",
    "href": "topic01_Python_Basics/script/script_01.html#footnotes",
    "title": "1  Python Basics for Data Science",
    "section": "",
    "text": "https://peps.python.org/pep-0008/↩︎",
    "crumbs": [
      "I Get",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Python Basics for Data Science</span>"
    ]
  },
  {
    "objectID": "appendix04_Probabilities/script/appendix_04.html",
    "href": "appendix04_Probabilities/script/appendix_04.html",
    "title": "Appendix A — Probabilities",
    "section": "",
    "text": "A.1 Probability, conditional probability, and dependence\nThis Appendix covers basic definitions and results of probabilities. It does not aim to replace a lecture on probabilities.\nWe indistinguishably denote \\(p(a)\\):\nThe joint probability of two events to occur (two random variables to take particular values) is denoted \\(p(a,b)\\).\nThe conditional probability of an event \\(a\\) given that \\(b\\) occurs, denoted \\(p(a|b)\\) and said “probability of a given b”, is defined as:\n\\[\\begin{align}\np(a|b):=\\frac{p(a,b)}{p(b)}\n\\end{align}\\]\nThe random variables \\(a\\) and \\(b\\) are independent, denoted \\(a \\perp b\\), if and only if \\[\\begin{align}\np(a,b) = p(a)p(b)\n\\end{align}\\] This equivalent to say that \\(p(a|b) = p(a)\\) and that \\(p(b|a) = p(b)\\).\nOtherwise, the random variables \\(a\\) and \\(b\\) are dependent, denoted \\(a \\not\\perp b\\).\nThese results generalize to discrete random variables and to probability densities of continuous random variables.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "appendix04_Probabilities/script/appendix_04.html#probability-conditional-probability-and-dependence",
    "href": "appendix04_Probabilities/script/appendix_04.html#probability-conditional-probability-and-dependence",
    "title": "Appendix A — Probabilities",
    "section": "",
    "text": "the probability of a logical event \\(A\\) to occur\nthe probability of a discrete random variable \\(A\\) to take the value \\(a\\)\nthe probability mass density of a continuous random variable \\(A\\) at the value \\(a\\)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "appendix04_Probabilities/script/appendix_04.html#expected-value-variance-and-covariance",
    "href": "appendix04_Probabilities/script/appendix_04.html#expected-value-variance-and-covariance",
    "title": "Appendix A — Probabilities",
    "section": "A.2 Expected value, variance, and covariance",
    "text": "A.2 Expected value, variance, and covariance\nIf \\(X\\) is a random variable with a probability density function \\(p(x)\\), then the expected value is defined as the sum (for discrete random variables) or integral (for univariate continuous random variables) 1: \\[\\operatorname{E}[X] = \\int x p(x)\\, dx\\]\nThe variance is defined as:\n\\[\\Var[X]=\\E[(X - \\E[X])^2]\\] The standard deviation is the squared root of the variance:\n\\[\\operatorname{SD}(X) = \\sqrt{\\Var(X)}\\]\nThe covariance of two random variables \\(X\\) and \\(Y\\) is defined as:\n\\[\\Cov[(X,Y)]=\\E[(X - \\E[X])(Y - \\E[Y])]\\] The Pearson correlation coefficient \\(\\rho_{X,Y}\\) of two random variables \\(X\\) and \\(Y\\) is defined as:\n\\[\\rho_{X,Y} = \\frac{\\Cov[(X,Y)]}{\\operatorname{SD}[X]\\operatorname{SD}[Y]}\\]\nThe expected value of multidimensional random variables is defined per component. That is,\n\\[\\E[(X_1,\\ldots,X_n)]=(\\E[X_1],\\ldots,\\E[X_n])\\]\nThe covariance matrix of a multidimensional random variables \\(X\\) is the matrix of all pairwise covariances, i.e. with \\((i,j)\\)-th element being:\n\\[(\\textbf{Cov}[X])_{i,j}=\\Cov[(X_i,X_j)]\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "appendix04_Probabilities/script/appendix_04.html#probs-sample-estimates",
    "href": "appendix04_Probabilities/script/appendix_04.html#probs-sample-estimates",
    "title": "Appendix A — Probabilities",
    "section": "A.3 Sample estimates",
    "text": "A.3 Sample estimates\nLet \\(\\{x_1,...,x_n\\}\\) a finite sample of size \\(n\\) of independent realizations of a random variable \\(X\\). Considered as random variables, the \\(x_i\\) are independently and identically distributed (i.i.d.).\nThe sample mean, often denoted \\(\\bar x\\), is defined as:\n\\[\\bar x = \\frac{1}{n}\\sum_i x_i\\] The sample mean is an unbiased estimator of the expected value. That is, \\(\\E[\\bar x] = \\E[X]\\).\nThe sample variance is defined as:\n\\[\\sigma^2_x = \\frac{1}{n}\\sum_i (x_i-\\bar x)^2\\] The sample variance is not an unbiased estimator of the variance. Therefore, one often uses the unbiased sample variance, defined as:\n\\[s_x^2 = \\frac{1}{n-1}\\sum_i (x_i-\\bar x)^2\\] for which \\(\\E[s^2_x] = \\Var[X]\\) holds.\nThe sample standard deviation and the unbiased sample standard deviation are defined as the squared root of their variance counterparts.\nThe sample Pearson correlation coefficient is given by:\n\\[\\begin{align}\nr =\\frac{\\sum ^n _{i=1}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum ^n _{i=1}(x_i - \\bar{x})^2} \\sqrt{\\sum ^n _{i=1}(y_i - \\bar{y})^2}}\n(\\#eq:pearson-r)\n\\end{align}\\]\nwhere \\(\\bar{x}=\\frac{1}{n}\\sum_{i=1}^n x_i\\) is the sample mean, and analogously for \\(\\bar{y}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "appendix04_Probabilities/script/appendix_04.html#appendix-lin-reg",
    "href": "appendix04_Probabilities/script/appendix_04.html#appendix-lin-reg",
    "title": "Appendix A — Probabilities",
    "section": "A.4 Linear regression",
    "text": "A.4 Linear regression\nThis is the proof for the univariate linear regression estimates.\nFor a data set \\((x, y)_i\\) with \\(i \\in \\{1 \\dots N\\}\\) the univariate linear model is defined as \\[y_i = \\alpha + \\beta x_i + \\epsilon_i\\] with free parameters \\(\\alpha\\) and \\(\\beta\\) and a random error \\(\\epsilon_i \\sim N(0, \\sigma^2)\\) that is i.i.d. (independently and identically distributed).\nThe normal distribution is defined as \\[N(\\epsilon | 0, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp(-\\frac{\\epsilon^2}{2 \\sigma^2}) .\\]\nThe assumption that the errors \\(\\epsilon_i\\) are independent and identically distributed allows us to factorize the Likelihood of the data under the linear model as \\[L(\\alpha, \\beta, \\sigma^2) = \\prod_{i=1}^{N} N(\\epsilon_i | 0, \\sigma^2) ,\\] using the fact that the probability of independent events is the product of the probabilities of each individual event.\nWe are interested in finding the parameters \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma^2\\) that best model our data. This can be achieved by finding the parameters that maximize the likelihood of our data. As maximizing the likelihood is equivalent to maximizing the log likelihood and as the log likelihood is easier to handle, we will use the log likelihood in the following.\nThe log likelihood of our data is defined as follows:\n\\[\\begin{align}\n\\log(L(\\alpha, \\beta, \\sigma^2)) & = \\log( \\prod_{i=1}^{N} N(\\epsilon_i | 0, \\sigma^2) ) \\\\\n& = \\sum_{i=1}^{N} \\log( N(y_i - (\\alpha + \\beta x_i) | 0, \\sigma^2) ) \\\\\n& =  - 0.5 N \\log(2 \\pi \\sigma^2) + \\sum_{i=1}^{N} - \\frac{(y_i - (\\alpha + \\beta x_i))^2}{2 \\sigma^2} .\n\\end{align}\\]\n\n\nWe can maximize a quadratic function by computing its gradient and setting it to zero, this yields: \\[\\hat{\\alpha} = \\bar{y} - \\hat{\\beta} \\bar{x}\\] \\[\\hat{\\beta} = \\frac{\\sum_{i=1}^N (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^N (x_i - \\bar{x})^2}\\] \\[\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^N (y_i - (\\hat{\\alpha} + \\hat{\\beta}x_i)^2)\\]\nwith means denoted by \\(\\bar{x}\\) and \\(\\bar{y}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "appendix04_Probabilities/script/appendix_04.html#resources",
    "href": "appendix04_Probabilities/script/appendix_04.html#resources",
    "title": "Appendix A — Probabilities",
    "section": "A.5 Resources",
    "text": "A.5 Resources\nThe chapters on probability and random variables of Rafael Irizzary’s book Introduction to Data Science Chapters gives related primer material [https://rafalab.github.io/dsbook/].",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  },
  {
    "objectID": "appendix04_Probabilities/script/appendix_04.html#footnotes",
    "href": "appendix04_Probabilities/script/appendix_04.html#footnotes",
    "title": "Appendix A — Probabilities",
    "section": "",
    "text": "This lose definition suffices for our purposes. Correct mathematical definitions of the expected value are involved. See https://en.wikipedia.org/wiki/Expected_value↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Probabilities</span>"
    ]
  }
]