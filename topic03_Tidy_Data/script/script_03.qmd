---
title: "Data Wrangling"
execute: 
  kernel: dataviz

format: html
# jupyter: python3

header-includes:
  - \setbeamerfont{section title}{size=\Large} 
  - \usepackage{float} \floatplacement{figure}{H} 
---
```{python}
#| echo: false
import numpy as np
import pandas as pd

# show a compact preview with ellipses
pd.set_option("display.max_rows", 12)     # total rows shown
pd.set_option("display.min_rows", 8)      # head + tail with "..."
pd.set_option("display.max_columns", 6)   # truncate columns with "..."
pd.set_option("display.width", 0)         # auto-detect width
pd.set_option("display.show_dimensions", True)  # optional (prints shape)
datadir = "../../extdata/"
```
<!-- # Tidy data and combining tables -->

## Introduction
### Motivation
Without good practices, much of the time of a data analyst can be wasted in data wrangling rather than visualization or analysis. The concept of tidy data [@Wickham2014] addresses this issue by offering a standard representation of data, that is easy to manipulate, model and visualize. This chapter introduces the notion of tidy data and operations for tidying up messy datasets. Moreover, we describe how to easily concatenate tables with the same format and merge tables with common variables. This will set us ready for data visualization and analytics.

This chapter is partially adopted from "Introduction to Data Science" by Rafael A. Irizarry (https://rafalab.github.io/dsbook/) and uses concepts and material introduced by the developers of the `tidyr` package.

### Datasets used in this chapter

The following code chunks load libraries and tables used throughout this chapter.

```{python, echo=TRUE, warning=FALSE}
import pandas as pd
from plotnine import *
from pathlib import Path
import glob
import numpy as np
# from pprint import pprint
```

```{python echo=TRUE, warning=FALSE}
DATADIR = Path('extdata')
election_results = pd.read_csv(DATADIR / 'US-pres16results.csv')
election_results = (
    election_results
    .loc[
        election_results["county"].isna() & (election_results["st"] != "US"),
        ["cand", "st", "votes", "total_votes"]
    ]
)
table1 = pd.read_csv(DATADIR / 'table1_alternate.csv')

table2 = pd.read_csv(DATADIR / 'table2_alternate.csv')

table3 = pd.read_csv(DATADIR / 'table3_alternate.csv')

table4 = pd.read_csv(DATADIR / 'table4_alternate.csv')

table5 = pd.read_csv(DATADIR / 'table5_alternate.csv')
```

## Tidy and untidy data

### Definition of tidy data

![**Tidy data table layout.** Each variable has a column, each observation a row and each value a cell.](assets/img/lec05_tidy-1_alternate.png)


We say that a data table is in  _tidy_ format if:
  
1. Each **variable** has its own **column**.
2. Each **observation** has its own **row**.
3. Each **value** has its own **cell**.

The following dataset from the 2016 US presidential vote^[https://www.kaggle.com/stevepalley/2016uspresidentialvotebycounty?select=pres16results.csv] is an example of a tidy dataset:


```{python, echo=FALSE, warning=FALSE}
election_results.head()
```

Each row represents a state and a candidate with each of the four values related to these states stored in the four variables: candidate, state, votes, and total_votes.

### Advantages of tidy data

Organizing data in a tidy fashion reduces the burden to frequently reorganize the data. In particular, the advantages are:

* Easier manipulation using `pandas` commands such as sub-setting by rows and columns, as well as `merge` operations
* Vectorized operations become easier to use
* Many other tools work better with tidy data, including plotting functions, hypothesis testing functions, and modeling functions such as linear regression. These advantages will become striking in the following chapters.

### Common signs of untidy datasets

Often, untidy datasets can be identified by one or more of the following issues [@Wickham2014]:
  
* Column headers are values, not variable names
* Multiple variables are stored in one column
* Variables are stored in both rows and columns
* A single observational unit is stored in multiple tables

@Wickham2014 furthermore mentions "Multiple types of observational units stored in the same table" as a sign of untidy data.
This point is discussed in Section \@ref(tidy-not-unique). 

<!-- We will show this advantages with the following example: -->

<!-- ```{python, echo = FALSE} -->
<!-- dt <- table1 -->
<!-- ``` -->
<!-- ```{python} -->
<!-- head(dt) -->
<!-- ``` -->

<!-- As stated before, tidy data can be easily manipulated. For example in the table above we can easily compute the percentage of votes given to a candidate in a particular state using the following commands: -->

<!--   ```{python} -->
<!-- # Compute percentage -->
<!-- dt[, percentage := votes / total_votes * 100] # vectorized operations; dt is modified -->
<!-- head(dt) -->

<!-- # Compute total votes per candidate -->
<!-- dt[, .(votes = sum(votes)), by = candidate] # note that this does not modify dt -->
<!-- ``` -->
<!-- ```{python, echo = FALSE} -->
<!-- dt_simple <- dt[candidate%in%c("Hillary Clinton", "Donald Trump") & state%in%c("CA", "FL")] -->
<!-- ``` -->

<!-- this would raise many questions, not the simplest plot, ggplot is complicated, etc. Also, a bit political. -->

<!-- Additionally, tidy data works better with many packages like ggplot2 which we are going to use in this course. -->
<!-- For the sake of simplicity of the plot we use data for the top candidates in selected states (California and Florida).  -->
<!-- ```{python leture05-ggplot-tidy-data-alternate} -->
<!-- ggplot(dt_simple, aes(state, votes, fill=candidate)) +  -->
<!--      geom_bar(position="dodge", stat="identity") -->
<!-- ``` -->

## Tidying up datasets

In this part of the chapter, we show how to transform untidy datasets into tidy ones. To this end, we will present some of the most often encountered untidy formats and present specific solutions to each of them.

### Melting (wide to long)

One of the most used operations to obtain tidy data is to transform a `wide` table into a `long` table. This operation is called melting, by analogy with `melting` a piece of metal. It is useful in particular when data is untidy because column headers are values, and not variable names.

As an example, consider the table below which reports vote counts for two US states, California and Florida. In this table, the column names CA and FL are values of the variable *state*. Therefore, we can say that this table is in an untidy format:

```{python}
table4
```

![Melting the election dataset](assets/img/lec05_tidy-gather_alternate.png)
Melting the previous table can be achieved by using the __pandas__ method `melt()`:

```{python}
pd.melt(table4, id_vars = "candidate", value_vars = ['CA', 'FL'],
     var_name = "state", value_name = "votes")
```

<!-- TODO maybe drop this?  -->
We note that the previous code snippet would also work without explicitly specifying `value_vars` or `id_vars`.
If `value_vars` is omitted, pandas will melt all columns not listed in id_vars â€”or all columns if `id_vars` is also not provided.
Conversely, if `id_vars` is omitted (and only value_vars is given), pandas will include only the columns defined in `value_vars`, resulting in an output that contains just the `var_name` and `value_name` columns.

When melting, all values in the columns specified by the `value_vars` argument are **gathered into one column** with the name specified in the `var_name` argument. 

Additionally, a new column with the name in the `value_name` argument is created **containing all values** which were previously stored in the `value_vars` **column names**.

After melting, we have a table in a tidy format where a row represents the number of votes for a candidate in a state. The new table also makes clear that the quantities are numbers of votes thanks to the column name.

### Pivoting (long to wide)

The other way around also happens frequently. It is helpful when multiple variables are stored in one column. In the table below, multiple values, namely the number of votes for a candidate and the total number of votes, are reported in one column. It is not easy to compute the percentage of votes given to a candidate in this format. To tidy up this table we have to separate those values into two columns:

```{python}
table2
```


![Casting the election dataset](assets/img/lec05_tidy-spread_alternate.png)

This operation, which transforms a long table into a wide table is called casting, following up with the metal forging analogy employed with the term "melting".

Pandas **pivot** transforms a long table into a wide table and is achieved with the `pivot` method whose most frequent usage is:

```{python, eval=FALSE}
wide = data.pivot(index=['<id>'], columns='<category>', 
values='<value>').reset_index()
```

The .pivot() method requires specifying three key arguments: `index`, `columns`, and `values`.

- `columns`: Specifies which column in the original (long) table contains the categories that will become the names of the new columns in the wide table.

- `values`: Refers to the column from which the data values should be extracted to populate the cells under the new wide columns.

- `index`: Specifies the column(s) that should form the new row indices (keys) of the wide table. (Note: Using reset_index() is necessary after the pivot to turn these indices back into regular columns.)

The call then is:

```{python}

table2.pivot(
    index=["candidate", "state"],   
    columns="type",              
    values="value"               
).reset_index()
```

<!-- The function `dcast` has many more arguments. Also the formula can allow for more sophisticated ways to handle the columns. We refer to the "datatable-reshape" vignette (see section Resources) and the help of `dcast()` for more details.  -->

### Separating columns

Sometimes single variables can be spread across multiple columns as in the following table.

```{python}
## One column contains multiple variables
table3
```

The number of votes per candidate is displayed in the numerator of the `proportion` column and the total number in the denominator.

We split up the `proportion` column into two columns, one containing the votes and the other one containing the total votes with the method `str.split`:

```{python}
table3[["votes", "total_votes"]] = table3["proportion"] \
  .str.split("/", expand=True)
table3
```

![Separated election dataset](assets/img/lec05_tidy-separate_alternate.png)
The drawing above visualizes the operation performed above.

### Uniting columns

In this example the first and last names are separated columns without a real need for it (we will not be interested in computing any statistics over all Hillary's):

```{python}
table5
```

We combine the contents of the name and surname columns into a single candidate column using the string addition operation `+`

```{python}
table5["candidate"] = table5["name"] + " " + table5["surname"]
```


![United election dataset](assets/img/lec05_tidy-unite_alternate.png)


### Advanced: Columns containing sets of values

Kaggle, a machine learning platform, conducts a yearly survey among its users. Below are a few columns of the answers from the 2017 survey. In those columns, we observe another type of untidy data. In this survey, multiple choice questions were asked from which multiple answers could be selected. For each individual the selected answers are concatenated into a string.

```{python}
survey = pd.read_csv('extdata/kaggle-survey-2017/multipleChoiceResponses.csv')
survey[['LanguageRecommendationSelect', 'LearningPlatformSelect', 'PastJobTitlesSelect']].head()
```



Below is one solution of how the `LearningPlatformSelect` column could be transformed into a tidy format. We make use here of the handy pipe operator denoted `%>%` from the  `magrittr` package (See Appendix \@ref(pipe)).

```{python}
platform_splits = survey['LearningPlatformSelect'].str.split(',', expand=True)
platform_splits['individual'] = survey.index
LearningPlatformMelt = platform_splits.melt(
    id_vars='individual', 
    value_name='Platform', 
    var_name=None           
)
LearningPlatformMelt.sort_values(by='individual').head(n=5)
```


## Concatenating tables

One frequently has to concatenate (i.e. append) tables with a same format. Such tables may already be loaded into a list or shall be read from multiple files.

For instance, assume a service generates a new file of data per day in a given directory. One is interested in analyzing the files of multiple days jointly. This requires to list all files of the directory, to read each file and to concatenate them into one.

Here is an example with daily COVID-19 data. We first get all file names of the directory into a character vector called `files`:

```{python, show = TRUE, eval=FALSE}
files = glob.glob('path_to_your_directory')
```
```{python, show = FALSE, echo=TRUE}
files = glob.glob("../../extdata/cov_concatenate/*")
files[:5]
```


Next, we load all file contents with `pd.read_csv` using a dictionary comprehension, which returns a dictiionary of `DataFrames` with filenames as the keys. 
```{python}
from pathlib import Path
tables = {Path(f).name: pd.read_csv(f) for f in files}
```

Let us now look at the first table: 
```{python}
tables['covid_cases_01_03_2020.csv'].head()
```

To combine the different tables into one, we can use the pandas function `pd.concat`. 
The `keys` argument introduces a MultiIndex, with the filenames forming the outermost index level. This allows us to keep information about the date when the data was acquired, which is stored in the filename. 
We then use `.reset_index(level='filename')` to convert this index level into a standard column named filename. The final `.reset_index(drop=True)` assigns a new, sequential index to the combined DataFrame.
```{python}
df = (
    pd.concat(tables.values(), keys=tables.keys(), 
    names=['filename', 'index'])
      .reset_index(level='filename')
      .reset_index(drop=True)
)
df.head()
```


## Merging tables

Merging two data tables into one by common column(s) is frequently needed. This can be achieved using the `merge` method whose core signature is:

```{python, eval=F}
df.merge(y, # tables to merge
        how = 'left', # type of merge
        on=None, left_on=None, right_on=None # by which columns
        )
```

The four types of merges (also commonly called joins) are:

* **Inner (default)**: consider only rows with matching values in the `by` columns.

* **Outer or full (all)**: return all rows and columns from `x` and `y`. If there are no matching values, return NAs.

* **Left (all.x)**: consider all rows from `x`, even if they have no matching row in `y`.

* **Right (all.y)**: consider all rows from `y`, even if they have no matching row in `x`.

We now provide examples of each type using the following made up tables: 

```{python}
df1 = pd.DataFrame({
    'p_id': ['G008', 'F027', 'L051'],
    'value': np.random.normal(size=3)
})
df1.head(2)

df2 = pd.DataFrame({
    'p_id': ['G008', 'F027', 'U093'],
    'country': ['Germany', 'France', 'USA']
})
df2.head(2)
```

### Inner merge

An inner merge returns only rows with matching values in the `by` columns and discards all other rows:
```{python}
# Inner merge, default one, all = FALSE
m = df1.merge(df2, on='p_id', how='inner')
m
```

<!-- Note that the row order got changed after the merging. To prevent this and, therefore, to keep the original ordering we can use the argument `sort` and set it to `FALSE`: 

```{python}
m <- merge(dt1, dt2, by = "p_id", all = FALSE, sort = FALSE)
m
``` -->

<!-- Note that the column order is preserved after merging. -->

### Outer (full) merge
An outer merge returns all rows and columns from `x` and `y`. If there are no matching values in `p_id`, it yields missing values (`NA`):

```{python}
# Outer (full) merge
m = df1.merge(df2, on='p_id', how='outer')
m
```

## Left merge
Returns all rows from `x`, even if they have no matching row in `y`. Rows from `x` with no matching `p_id` in `y` lead to missing values (`NA`).
```{python}
# Left merge 
m = df1.merge(df2, on='p_id', how='left')
m
```


## Right merge
Returns all rows from `y`, even if they have no matching row in `x`. Rows from `y` with no matching `p_id` in `x` lead to missing values (`NA`).
```{python}
# Right merge
m = df1.merge(df2, on='p_id', how='right')
m
```

### Merging by several columns
Merging can also be done using more than two columns. Here are two made up tables to illustrate this use case:

```{python}
df1 = pd.DataFrame({
    'firstname': ['Alice', 'Alice', 'Bob'],
    'lastname':  ['Coop',  'Smith', 'Smith'],
    'x': [1, 2, 3]
})
df1.head(2)

df2 = pd.DataFrame({
    'firstname': ['Alice', 'Bob',   'Bob'],
    'lastname':  ['Coop',  'Marley','Smith'],
    'y': list('ABC')
})
df2.head(2)
```

We now merge `df1` and `df2` by first name and last name:
```{python}
df1.merge(df2, on=['firstname', 'lastname'])
```

Note that merging by first name only gives a different result (as expected):

```{python}
df1.merge(df2, on='firstname')
```


Also notice that in this case the merge tables has a column `lastname_x` and a column `lastname_y`. This is because the two original data tables have a column named the same way ("lastname"), but this column was not part of the "by" argument. Hence, it is assumed that they do not necessarily correspond to the same variable. Hence, they receive distinct names in the returned table. 

## Tidy representations are not unique {#tidy-not-unique}
While untidy data should be avoided, there can be multiple tidy representations for a particular dataset. We explain this regarding i) alternative forms of a single table and ii) the practical utility of non-normalized representations (i.e. with redundant information). 

### Alternative tidy forms of a table
There can be alternative tidy representations for a same table.
Here is an example based on Fisher's Iris dataset. This classic dataset contains measurements of 4 different attributes for 150 iris flowers from 3 different species. See <https://en.wikipedia.org/wiki/Iris_flower_data_set>.

```{python echo=F, fig.show = "hold", out.width = "40%", fig.align = "center", fig.pos='H'}
knitr::include_graphics("assets/img/lec03_iris.png")
```

```{python, echo = F}
from sklearn import datasets

iris = datasets.load_iris(as_frame=True)
iris_df = iris.frame.rename(columns={'target': 'Species'})
iris_df['Species'] = iris_df['Species'].map(dict(enumerate(iris.target_names)))
iris_df.insert(0, 'Flower', [f"F_{i+1}" for i in range(len(iris_df))])

iris_melt = pd.melt(
    iris_df,
    id_vars=['Flower', 'Species'],
    var_name='Attribute',
    value_vars=['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
)
```

Here is one tidy representation where each row represents one flower:
```{python, echo = T, results = T}
# Iris dataset, usual representation
iris_df.head(3)
```

Here is another tidy representation where each row represents one measurement:
```{python, echo = T, results = T}
# Another tidy representation
iris_melt.head(3)
```

Both representations are tidy and can be more or less useful depending on the downstream analysis. For instance the first representation, which is wide, is handy to assess the relationship between sepal length and sepal width, say by plotting one against the other one or by computing their correlations. The second representation, which is long, can be useful to compute means by attributes or by attributes and species. In the wide form, computing those group means would require to select columns by names which is tedious and leads to not-easily maintainable code. The decisive criteria between using one or the other tidy representation is the definition on what is considered as an observation in the use case.


### On multiple types of observational units in the same table

Another important remark for handling tidy data in practice relates to the last common sign of messy datasets according to @Wickham2014, i.e. "Multiple types of observational units are stored in the same table". Applying this criteria actually depends on the context.

Consider the following table which combines product and customer data:  
```{python, echo = F, results = T}
prod_df = pd.DataFrame({
    "productCode": ["p018", "p030", "p018"],
    "productName": ["dryer", "phone", "dryer"],
    "customerNumber": ["c001", "c001", "c002"],
    "customerName": ["Smith", "Smith", "Lewis"],
    "priceEach": [450, 600, 450],
    "state": ["CA", "CA", "AZ"],
    "quantOrdered": [1, 2, 1]
})

prod_df[["productCode", "quantOrdered", "priceEach",
         "customerNumber", "customerName", "state"]]
```

This table is tidy. Each row corresponds to an order. The columns are variables. However, it contains repetitive information: the product code, product name and its price on the one hand, the customer number, name and state on the other hand. The information could be stored in separate tables without data repetitions, namely:

* a consumer table:
```{python, echo = F, results = T}
prod_df[['customerNumber', 'customerName', 'state']].drop_duplicates()
```

* a product table:
```{python, echo = F, results = T}
prod_df[['productCode', 'price']].drop_duplicates()
```

* an order table:
```{python, echo = F, results = T}
prod_df[['productCode', 'customerNumber', 'quantOrdered']].drop_duplicates()
```

The three-table representation, where each table has unique entries is called a normalized representation. Normalized representations ensure that no multiple types of observational units are stored in the same table. It is a good habit to have normalized representations for database back-ends because it facilitates maintenance of the data consistency by reducing redundancy. One should not enter all customer details at each order but do it one central place and link the information with a customer number.

However, on the data analysis side  (front-end), we are not interested in maintaining a database (back-end), rather in having the desired data in a ready-to-use format which depends on our needs. To this end, the merge table is very handy and can be the common denominator of multiple analyses like:
```{python}
# vectorized operations e.g. total price of each order
prod_df["totalPrice"] = prod_df["quantOrdered"] * prod_df["priceEach"]

# group by operations, e.g. number of products per states 
prod_df["N_prod"] = prod_df.groupby("state")["productCode"].transform("count")
```

Hence, the choice of the representation (normalized or not) depends on the context: back-end or front-end.

## Summary
By now, you should be able to:

* define what a tidy dataset is
* recognize untidy data
* perform the operations of melting and casting
* perform the operations of uniting and splitting
* append tables with the same format by rows
* understand and perform the 4 merging operations

## Tidy data resources
Tidy data:
H. Wickham, Journal of Statistical Software, 2014, Volume 59, Issue 10 <https://www.jstatsoft.org/v59/i10/paper>

Pandas:
<https://pandas.pydata.org/docs/index.html>

<!-- Stringr, A complementary package for string manipulations: -->
<!-- <https://github.com/rstudio/cheatsheets/raw/master/strings.pdf> -->

<!-- ### Advantages of non-tidy data -->

<!-- * Performance advantage using certain functions -->
<!--   + `colSums()` or `heatmap()` on matrices -->

<!-- * Field convention -->

<!-- * Memory efficiency -->

<!--   + don't worry, you should be fine with tidy-data in `data.table` -->

<!-- Interesting blog post: -->

<!--   * <http://simplystatistics.org/2016/02/17/non-tidy-data/> -->



